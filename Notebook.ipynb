{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahJw4U2fAKqF"
   },
   "source": [
    "# Bayesian Statistics Project\n",
    "By: Georges SARR - Lasme Ephrem ESSOH - Hamdi BEL HADJ HASSINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNqLblAjlhcO",
    "outputId": "77ef99e4-5cf6-4a0d-d5b1-02e1257cc170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxpy in /opt/conda/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: scs>=1.1.6 in /opt/conda/lib/python3.7/site-packages (from cvxpy) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from cvxpy) (1.19.4)\n",
      "Requirement already satisfied: osqp>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from cvxpy) (0.6.2.post5)\n",
      "Requirement already satisfied: ecos>=2 in /opt/conda/lib/python3.7/site-packages (from cvxpy) (2.0.10)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from cvxpy) (1.7.3)\n",
      "Requirement already satisfied: qdldl in /opt/conda/lib/python3.7/site-packages (from osqp>=0.4.1->cvxpy) (0.1.5.post0)\n"
     ]
    }
   ],
   "source": [
    "# Install/Import packages\n",
    "! pip install cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UZRs_1w8AN6B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.stats import beta as beta_distribution\n",
    "from numpy.random import gamma as gamma_distribution\n",
    "from numpy.random import uniform as uniform_distribution\n",
    "from numpy.random import multivariate_normal\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import scipy\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frt4M-MqAPBZ"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to replicate the paper's results so we generate the same example datasets as the first 4 ones used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgABdTwTWOs7"
   },
   "outputs": [],
   "source": [
    "def get(data,g, beg):\n",
    "  '''\n",
    "  # INPUT:         # data is a dictionary of key in [\"X\", \"beta\"] if key=\"X\" then data should be the data of covariates\n",
    "                     otherwise it should be an np.array of the parameter beta\n",
    "                   # g is the number of the group to retrieve\n",
    "                   # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "\n",
    "  # OUTPUT:        # Returns the gth group of X/beta , example get({\"X\":np.array([...])},0,beg) retrievs the first group of X\n",
    "  '''\n",
    "  assert type(data)==dict\n",
    "  assert beg[0]==0 and len(beg)>=2\n",
    "  assert 0<=g<len(beg)\n",
    "  if list(data.keys())[0]==\"X\":\n",
    "    if g<len(beg)-1:\n",
    "      return data[\"X\"][:, beg[g]:beg[g+1]]\n",
    "    else:\n",
    "      return data[\"X\"][:, beg[g]:]\n",
    "  elif list(data.keys())[0]==\"beta\":\n",
    "    if g<len(beg)-1:\n",
    "      return data[\"beta\"][beg[g]:beg[g+1]]\n",
    "    else:\n",
    "      return data[\"beta\"][beg[g]:]\n",
    "\n",
    "def lgroup(g,beg, nb_covariates):\n",
    "  '''\n",
    "  # INPUT:       # g is the number of the group to retrieve\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # nb_covariates is the total number of features/covariates\n",
    "\n",
    "  # OUTPUT:      # gives the length of the gth group\n",
    "  '''\n",
    "  assert beg[0]==0 and len(beg)>=2\n",
    "  assert 0<=g<len(beg)\n",
    "  if g<len(beg)-1:\n",
    "    return beg[g+1]-beg[g]\n",
    "  else:\n",
    "    return nb_covariates-beg[g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Y0OkmS_zaQOe"
   },
   "outputs": [],
   "source": [
    "### examples\n",
    "nb_sims=50 # number of replications for each example\n",
    "\n",
    "# parametrizing the examples\n",
    "example={1: {\"beta\": np.array([0.3,-1,0,0.5,0.01]+[0]*5+[0.8]*5+[0]*5), # true parameter\n",
    "             \"n\":100,  # number of observations\n",
    "             \"p\":20,   # number of covariates\n",
    "             \"sigma\":3, # the standard deviation of the residuals\n",
    "             \"beg\":list(range(0,20,5)), # the indices of the beginning of the groups (the first group starts at index 0 and ends at index 4, \n",
    "                                        # the next one starts at index 5 and ends at index 9, etc)\n",
    "             \"sim\":dict() # will store the simulations of X and Y                                       \n",
    "             },\n",
    "         \n",
    "         2: {\"beta\": np.array([1,2,3,4,5]+[0]*5+[0.1,0.2,0.3,0.4,0.5]+[0]*(13*5)),\n",
    "             \"n\": 60,\n",
    "             \"p\": 80,\n",
    "             \"sigma\": 2,\n",
    "             \"beg\": [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75],\n",
    "             \"sim\": dict()},\n",
    "         \n",
    "         3: {\"beta\": np.array([0]*10+[2]*10+[0]*10+[2]*10),\n",
    "             \"n\": 100,\n",
    "             \"p\": 40,\n",
    "             \"sigma\": 2,\n",
    "             \"beg\": [0,10,20,30],\n",
    "             \"sim\": dict()\n",
    "             },\n",
    "         \n",
    "         4: {\"beta\": np.array([0]*10+[2]*5+[0]*5+[0]*10+[2]*5+[0]*5),\n",
    "             \"n\": 100,\n",
    "             \"p\": 40,\n",
    "             \"sigma\": 2,\n",
    "             \"beg\": [0,10,15,20,30,35],\n",
    "             \"sim\": dict()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# simulating the examples\n",
    "\n",
    "## example 1\n",
    "for k in range(nb_sims):\n",
    "  epsilon=np.random.multivariate_normal([0]*example[1][\"n\"], (example[1][\"sigma\"]**2)*np.eye(example[1][\"n\"]))\n",
    "  mean=[0]*example[1][\"p\"]\n",
    "  cov=0.5*( np.ones((example[1][\"p\"],example[1][\"p\"])) + np.eye(example[1][\"p\"]) )\n",
    "  example[1]['sim'][k]={\"X\":np.random.multivariate_normal(mean, cov, example[1][\"n\"])}\n",
    "  example[1]['sim'][k][\"Y\"]=example[1]['sim'][k][\"X\"]@example[1]['beta']+epsilon \n",
    "\n",
    "## examples 2, 3 and 4\n",
    "def simulate(exple, number):\n",
    "  '''\n",
    "  Simulating example 2, 3, and 4\n",
    "  '''\n",
    "  for k in range(nb_sims):\n",
    "    n,p, sigma=exple[number][\"n\"],exple[number][\"p\"],exple[number][\"sigma\"]\n",
    "    epsilon=np.random.multivariate_normal([0]*n, (sigma**2)*np.eye(n))\n",
    "    beg=exple[number][\"beg\"]\n",
    "    X=[]\n",
    "    for g in range(len(beg)):\n",
    "      m_g=lgroup(g,beg, p)\n",
    "      z_g = np.random.multivariate_normal([0]*n, np.eye(n))\n",
    "      z_gs = np.stack([z_g for _ in range(m_g)])\n",
    "      z_gjs = np.random.multivariate_normal([0]*n, np.eye(n), m_g)\n",
    "      X_g = z_gs + z_gjs\n",
    "      X_g = X_g.T\n",
    "      X.append(X_g)\n",
    "    X=np.concatenate(X, axis=1)\n",
    "    exple[number]['sim'][k] = {\"X\":X}\n",
    "    exple[number]['sim'][k][\"Y\"] = X@exple[number]['beta']+epsilon\n",
    "\n",
    "for number in [2,3,4]:\n",
    "  simulate(example, number)\n",
    "\n",
    "beta_star=example[1][\"beta\"]\n",
    "X=example[1][\"sim\"][0][\"X\"]\n",
    "Y=example[1][\"sim\"][0][\"Y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEpiM0-5bsgw"
   },
   "source": [
    "# Group Lasso\n",
    "\n",
    "The objective is to solve (1)\n",
    "\\begin{equation} \n",
    "\\min_{\\beta \\in \\mathbb{R}^p} || Y - X \\beta ||_2^2 + \\lambda \\sum_{g=1}^G||\\beta_g||_2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta=(\\beta_1',...,\\beta_G')' \\in \\mathbb{R}^p$ and $\\beta_g \\in \\mathbb{R}^{m_g}$ such that $\\sum_{g=1}^G m_g = p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "MnHxOurMmIXv"
   },
   "outputs": [],
   "source": [
    "def train_test_split(X,Y,train_threshold=0.8):\n",
    "  '''\n",
    "  ## INPUT:       # X the covariates data\n",
    "                  # Y the labels data\n",
    "                  # train_threshold is the percentage of the data that make up the training data\n",
    "  \n",
    "  ## OUTPUT:      # The training and validation sets\n",
    "  '''\n",
    "  n=X.shape[0]\n",
    "  perm_ids=np.random.permutation(n)\n",
    "  train_ids=perm_ids[:int(n*train_threshold)] \n",
    "  val_ids=perm_ids[int(n*train_threshold):]\n",
    "  Xtrain, Xval = X[train_ids], X[val_ids]\n",
    "  Ytrain, Yval = Y[train_ids], Y[val_ids]\n",
    "\n",
    "  return Xtrain,Xval,Ytrain,Yval\n",
    "\n",
    "\n",
    "\n",
    "def run_group_lasso(Xtrain, Ytrain, Xval, Yval, lamb, beg, val_mse, solver):\n",
    "  '''\n",
    "  ## INPUT:        # Xtrain is the training data covariates\n",
    "                   # Ytrain is the training data labels\n",
    "                   # Xval is the validation data covariates\n",
    "                   # Yval is the validation data labels\n",
    "                   # lamb is the Group Lasso penalty coefficient\n",
    "                   # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                   # val_mse is a list for validation mean squared errors\n",
    "                   # solver is the solver used to solve the optimization problem\n",
    "\n",
    "  ## OUTPUT:       # The validation mse updated and one solution (= to None if the solver did not find a solution)\n",
    "  '''\n",
    "  p=Xtrain.shape[1]\n",
    "  beta=cp.Variable(p)\n",
    "  if len(beg)>1:\n",
    "    cost = sum([cp.sum_squares(Ytrain-Xtrain@beta),\n",
    "                lamb*sum([cp.norm(beta[beg[i]:beg[i+1]],2) for i in range(len(beg)-1)]),\n",
    "                lamb*cp.norm(beta[beg[-1]:],2)\n",
    "              ])\n",
    "  if len(beg)==1:\n",
    "    # ridge penalty case\n",
    "    cost = sum([cp.sum_squares(Ytrain-Xtrain@beta), lamb*cp.norm(beta,2)])\n",
    "  prob = cp.Problem(cp.Minimize(cost))\n",
    "  try:\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"ignore\")\n",
    "      # solve the problem\n",
    "      prob.solve(solver=solver)\n",
    "      solution=beta.value\n",
    "  except: solution=None # No solution found\n",
    "  \n",
    "  # updating the list of validation mse if there is a validation set\n",
    "  if solution is not None and len(Yval)>0 and len(Xval)>0:\n",
    "    val_mse.append(np.linalg.norm(Yval-Xval@solution,2)**2/len(Yval))\n",
    "\n",
    "  return val_mse, solution\n",
    "\n",
    "\n",
    "def optimal_group_lasso(X,Y,lambdas, beg,train_threshold=0.8, nb_runs=50, solvers=[\"ECOS\"]):\n",
    "  '''\n",
    "  ## INPUT:         # X is the matrix of covariates\n",
    "                    # Y is the vector of labels\n",
    "                    # lambdas is a list of Group Lasso penalty coefficient\n",
    "                    # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                    # train_threshold designates the proportion of data X,Y used for training\n",
    "                    # nb_runs is the number of times the solvers are run for each lambda in lambdas\n",
    "                    # solvers is a list of solvers used to solve the group lasso optimization problem using cvxpy package\n",
    "  \n",
    "  ## OUTPUT:        # returns the \"optimal\" Group Lasso penalty coeffient for each cvxpy solver along with the validation sets mean squared errors for \n",
    "                      each solver and lambda\n",
    "  '''\n",
    "  if beg[0]!=0:\n",
    "    raise ValueError (\"beg should be a list like object with 0 as the first element\")\n",
    "\n",
    "  mse=dict()\n",
    "  opt_lambda=dict()\n",
    "  n,p=X.shape\n",
    "\n",
    "  for solver in solvers:\n",
    "    mse[solver]=dict()\n",
    "    opt_lambda[solver]=dict()\n",
    "    print(solver ,\":\")\n",
    "    for lamb in tqdm(lambdas):\n",
    "      val_mse=[]\n",
    "      # running the optimization nrun times after permuting the data each time\n",
    "      for _ in range(nb_runs):\n",
    "        Xtrain,Xval,Ytrain,Yval=train_test_split(X,Y,train_threshold=0.8)\n",
    "        # solving the optimization problem\n",
    "        val_mse, _ = run_group_lasso(Xtrain, Ytrain, Xval, Yval, lamb, beg, val_mse, solver)\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        mse[solver][lamb]=val_mse\n",
    "        # metric = standard deviation adjusted mean mse\n",
    "        opt_lambda[solver][lamb]=np.mean(val_mse)+np.std(val_mse) \n",
    "\n",
    "    # optimal lambda and metric for a given solver\n",
    "    opt_lambda[solver]=sorted(opt_lambda[solver].items(), key=lambda item: item[1], reverse=False)[0]\n",
    "\n",
    "  # optimal solver: each solver is associated to its optimal adjusted mean mse\n",
    "  solver_opt_metric = {solver: opt_lambda[solver][1] for solver in opt_lambda}\n",
    "  opt_solver = sorted(solver_opt_metric.items(), key=lambda item: item[1], reverse=False)[0][0]\n",
    "\n",
    "  return mse, {opt_solver: opt_lambda[opt_solver][0]}\n",
    "\n",
    "\n",
    "def plot_lambda_vs_mse(lambdas, mse, nb_runs, solver='ECOS'):\n",
    "  plt.figure(figsize=(20,10))\n",
    "  a=[l for l in lambdas for _ in range(len(mse[solver][l]))]\n",
    "  b=np.concatenate([mse[solver][l] for l in lambdas], axis=0)\n",
    "  sb.boxplot(x=a, y=b)\n",
    "  plt.xlabel(\"$\\lambda$\")\n",
    "  plt.ylabel(\"MSE\")\n",
    "  plt.title(f\"Mean Squared Error on the validation sets with {nb_runs} runs using solver {solver}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPM61YSXNuTW",
    "outputId": "7de9fd7b-9667-4351-fe9b-c55b8472a5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVXOPT :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOS :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "lambdas=[1e-6, 1e-4, 1e-2, 1, 10, 100, 1000, 1e4, 1e5, 1e6]\n",
    "nb_runs = 100      \n",
    "train_threshold=0.6\n",
    "solvers=[\"CVXOPT\", \"ECOS\"]\n",
    "beg=example[1][\"beg\"]\n",
    "\n",
    "mse, opt_lambda=optimal_group_lasso(X, Y, lambdas, beg, train_threshold=train_threshold, nb_runs=nb_runs, solvers=solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvuRdU0nUHdD",
    "outputId": "b9a963b5-2975-4f8c-dd3c-81e7b8e5eb00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CVXOPT': 10}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "7T2qvE5ioqeX",
    "outputId": "978c6e36-c8fd-451d-aa4b-dbf5aaa4fdc5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_lambda_vs_mse(lambdas, mse, nb_runs, solver=list(opt_lambda.keys())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLfHklh7QOQM"
   },
   "source": [
    "Solving the problem with the optimal $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbR_8kYQPGQY",
    "outputId": "12c16d3d-a03b-4c7e-da16-fe1f0bcf278e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR=1.0 , FPR=1.0\n"
     ]
    }
   ],
   "source": [
    "lambda_star=list(opt_lambda.values())[0]  # optimal lambda\n",
    "eps=1e-6 # represents the tolerance to agree on equality of two float numbers\n",
    "\n",
    "Xtrain,Xval,Ytrain,Yval=train_test_split(X,Y,train_threshold=0.8)\n",
    "_, solution = run_group_lasso(X, Y, [], [], lambda_star, beg, [], list(opt_lambda.keys())[0])\n",
    "\n",
    "if solution is not None:\n",
    "    # True Positive Rate = proportion of non-zero coefficients correctly detected \n",
    "    TP=np.where((abs(beta_star)>eps) & (abs(solution)>eps))[0]\n",
    "    # False positives: null coefficients that were estimated as non-zero\n",
    "    FP=np.where((abs(beta_star)<eps) & (abs(solution)>eps))[0]\n",
    "    # true zeros are the coordinates of zeros in the true solution\n",
    "    true_zeros=np.where(beta_star==0)[0]\n",
    "    # true non zeros are the non zeros coordinates of the true solution\n",
    "    true_non_zeros=np.where(beta_star!=0)[0] \n",
    "    TPR=len(TP)/len(true_non_zeros)\n",
    "    # FPR=FP/Negatives\n",
    "    FPR=len(FP)/len(true_zeros)\n",
    "\n",
    "    print(f\"TPR={round(TPR,2)} , FPR={round(FPR,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some predictions and true Y values for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=example[1][\"sim\"][0][\"X\"]\n",
    "Y=example[1][\"sim\"][0][\"Y\"]\n",
    "Xtrain,Xval,Ytrain,Yval=train_test_split(X,Y,train_threshold=0.8)\n",
    "val_mse = []\n",
    "beg=example[1][\"beg\"]\n",
    "beta_star=example[1][\"beta\"]\n",
    "lamb = 100\n",
    "solver = \"ECOS\"\n",
    "p=Xtrain.shape[1]\n",
    "beta=cp.Variable(p)\n",
    "lamb = 100\n",
    "\n",
    "cost = sum([cp.sum_squares(Ytrain-Xtrain@beta),\n",
    "            lamb*sum([cp.norm(beta[beg[i]:beg[i+1]],2) for i in range(len(beg)-1)]),\n",
    "            lamb*cp.norm(beta[beg[-1]:],2)])\n",
    "prob = cp.Problem(cp.Minimize(cost))\n",
    "\n",
    "prob.solve(solver=solver)\n",
    "solution=beta.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672.5856888274578"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for the true beta (training data)\n",
    "sum([np.linalg.norm(Ytrain-Xtrain@beta_star,2)**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663.8671220358722"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for beta_hat (training data)\n",
    "sum([np.linalg.norm(Ytrain-Xtrain@solution,2)**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.336773973703751"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for the true beta (testing data)\n",
    "sum([np.linalg.norm(Yval-Xval@beta_star,2)**2])/len(Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.313807026177546"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for beta_hat (testing data)\n",
    "sum([np.linalg.norm(Yval-Xval@solution,2)**2])/len(Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2fa8486d90>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Xtrain@solution, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZtSfyYRO3Za"
   },
   "source": [
    "Now let us do the whole process for all the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr_group_lasso_one_simulation(exple, sim_number, lambdas, train_threshold, solvers, eps=1e-6):\n",
    "    '''\n",
    "    ## INPUT:       # exple is a given example made of simulations\n",
    "                  # lambdas is a list of Group Lasso penalty coefficient\n",
    "                  # train_threshold designates the proportion of data X,Y used for training\n",
    "                  # solvers is a list of solvers used to solve the group lasso optimization problem using cvxpy package\n",
    "                  # eps is the tolerance to agree on equality on two float numbers\n",
    "\n",
    "    ## OUTPUT:      # The True Positive and False Positive Rates\n",
    "    '''\n",
    "    # the data\n",
    "    beg=exple[\"beg\"]\n",
    "    beta_star=exple[\"beta\"]\n",
    "    X=exple[\"sim\"][sim_number][\"X\"]\n",
    "    Y=exple[\"sim\"][sim_number][\"Y\"]\n",
    "    Xtrain,Xval,Ytrain,Yval=train_test_split(X,Y,train_threshold=train_threshold)\n",
    "\n",
    "    # fine tuning lambda\n",
    "    mse, opt_lambda=optimal_group_lasso(Xtrain, Ytrain, lambdas, beg, train_threshold=train_threshold, nb_runs=nb_runs, solvers=solvers)\n",
    "    opt_solver, lambda_star = list(opt_lambda.items())[0]\n",
    "\n",
    "    # solving with the optimal lambda, on the whole dataset (without validating)\n",
    "    MSE, solution = run_group_lasso(Xtrain, Ytrain, Xval, Yval, lambda_star, beg, [], opt_solver)\n",
    "\n",
    "    TPR,FPR = np.nan, np.nan\n",
    "    if solution is not None:\n",
    "        # True Positive Rate = proportion of non-zero coefficients correctly detected \n",
    "        TP=np.where((abs(beta_star)>eps) & (abs(solution)>eps))[0]\n",
    "        # False positives: null coefficients that were estimated as non-zero\n",
    "        FP=np.where((abs(beta_star)<eps) & (abs(solution)>eps))[0]\n",
    "        # true zeros are the coordinates of zeros in the true solution\n",
    "        true_zeros=np.where(beta_star==0)[0]\n",
    "        # true non zeros are the non zeros coordinates of the true solution\n",
    "        true_non_zeros=np.where(beta_star!=0)[0] \n",
    "        TPR=len(TP)/len(true_non_zeros)\n",
    "        # FPR=FP/Negatives\n",
    "        FPR=len(FP)/len(true_zeros)\n",
    "    return((TPR,FPR,MSE[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kINjjq3bT7DY",
    "outputId": "d558f3f7-c654-4a13-b9f0-7d82cd0f3565",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n"
     ]
    }
   ],
   "source": [
    "lambdas=[1e-2, 1, 10, 100, 1000, 1e4]\n",
    "nb_runs = 50\n",
    "train_threshold=0.6\n",
    "solvers=[\"CVXOPT\", \"ECOS\"]\n",
    "\n",
    "results_tpr_fpr_mse = []#pd.DataFrame(columns=[\"Mean TPR\", \"STD TPR\" ,\"Mean FPR\", \"STD FPR\"])\n",
    "for number in example:\n",
    "    print(f\"Example {number}\")\n",
    "    exple=example[number]\n",
    "    nb_sims=len(exple[\"sim\"])\n",
    "    results = Parallel(n_jobs=10)(delayed(tpr_fpr_group_lasso_one_simulation)\n",
    "        (exple, sim_number, lambdas, train_threshold, solvers)\n",
    "        for sim_number in range(nb_sims))\n",
    "    TPRs = [x[0] for x in results if x!=np.nan]\n",
    "    FPRs = [x[1] for x in results if x!=np.nan]\n",
    "    MSEs = [x[2] for x in results if x!=np.nan]\n",
    "    results_tpr_fpr_mse.append([np.mean(TPRs), np.std(TPRs), np.mean(FPRs), np.std(FPRs), np.median(MSEs), np.std(MSEs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "\n",
      "    Mean TPR=0.9022222222222223, STD TPR=0.1841094682418487\n",
      "\n",
      "    Mean FPR=0.5054545454545455, STD FPR=0.4013036607099087\n",
      "\n",
      "    Median MSE=10.224343533226822, STD MSE=2.489460225131496\n",
      "\n",
      "Example 2:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=0.4217142857142857, STD FPR=0.3534002760356036\n",
      "\n",
      "    Median MSE=6.552892470753012, STD MSE=2.222006791932855\n",
      "\n",
      "Example 3:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=0.55, STD FPR=0.33734255586866\n",
      "\n",
      "    Median MSE=6.002643300534867, STD MSE=1.3112246487714037\n",
      "\n",
      "Example 4:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=0.5473333333333333, STD FPR=0.20397058611476315\n",
      "\n",
      "    Median MSE=4.8696089502278435, STD MSE=1.3494361006521038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for number in example:\n",
    "    results=results_tpr_fpr_mse[number-1]\n",
    "    print(f\"\"\"Example {number}:\\n\n",
    "    Mean TPR={results[0]}, STD TPR={results[1]}\\n\n",
    "    Mean FPR={results[2]}, STD FPR={results[3]}\\n\n",
    "    Median MSE={results[4]}, STD MSE={results[5]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QzmgbJN_iL_"
   },
   "source": [
    "# Sparse Group Lasso\n",
    "\n",
    "Solving \n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} || Y - X \\beta ||_2^2 + \\lambda_1 ||\\beta||_1 + \\lambda_2 \\sum_{g=1}^G||\\beta_g||_2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta=(\\beta_1',...,\\beta_G')' \\in \\mathbb{R}^p$ and $\\beta_g \\in \\mathbb{R}^{m_g}$ such that $\\sum_{g=1}^G m_g = p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "RrL9XonibrLe"
   },
   "outputs": [],
   "source": [
    "def run_sparse_group_lasso(Xtrain, Ytrain, Xval, Yval, lamb1, lamb2, beg, val_mse, solver):\n",
    "  '''\n",
    "  ## INPUT:        # Xtrain is the training data covariates\n",
    "                   # Ytrain is the training data labels\n",
    "                   # Xval is the validation data covariates\n",
    "                   # Yval is the validation data labels\n",
    "                   # lamb1  is the sparsity penalty coefficient (L_1 norm)\n",
    "                   # lamb2  is the Group Lasso penalty coefficient (L_2 norm)\n",
    "                   # lamb is the Group Lasso penalty coefficient\n",
    "                   # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                   # val_mse is a list for validation mean squared errors\n",
    "                   # solver is the solver used to solve the optimization problem\n",
    "\n",
    "  ## OUTPUT:       # The validation mse updated and one solution (= to None if the solver did not find a solution)\n",
    "  '''\n",
    "  p=Xtrain.shape[1]\n",
    "  beta=cp.Variable(p)\n",
    "  if len(beg)>1:\n",
    "    cost = sum([cp.sum_squares(Ytrain-Xtrain@beta), lamb1*cp.norm(beta,1),\n",
    "                        lamb2*sum([cp.norm(beta[beg[i]:beg[i+1]],2) for i in range(len(beg)-1)]), \n",
    "                        lamb2*cp.norm(beta[beg[-1]:],2)\n",
    "               ])\n",
    "  if len(beg)==1:\n",
    "    # generalised elastic net case\n",
    "    cost = sum([cp.sum_squares(Ytrain-Xtrain@beta), lamb1*cp.norm(beta,1), lamb2*cp.norm(beta,2)])\n",
    "  prob = cp.Problem(cp.Minimize(cost))\n",
    "  try:\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"ignore\")\n",
    "      # solve the problem\n",
    "      prob.solve(solver=solver)\n",
    "      solution=beta.value\n",
    "  except: solution=None # No solution found\n",
    "  \n",
    "  # updating the list of validation mse if there is a validation set\n",
    "  if solution is not None and len(Yval)>0 and len(Xval)>0:\n",
    "    val_mse.append(np.linalg.norm(Yval-Xval@solution,2)**2/len(Yval))\n",
    "\n",
    "  return val_mse, solution\n",
    "\n",
    "def optimal_sparse_group_lasso(X,Y,lambdas1, lambdas2, beg,train_threshold=0.6, nb_runs=50, solvers=[\"ECOS\"]):\n",
    "  '''\n",
    "  ## INPUT:         # X is the matrix of covariates\n",
    "                    # Y is the vector of labels\n",
    "                    # lambdas1  is a list of sparsity penalty coefficient (L_1 norm)\n",
    "                    # lambdas2  is a list of group penalty coefficient (L_2 norm)\n",
    "                    # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                    # train_threshold designates the proportion of data X,Y used for training\n",
    "                    # nb_runs is the number of times the solvers are run for each lambda in lambdas\n",
    "                    # solvers is a list of solvers used to solve the group lasso optimization problem using cvxpy package\n",
    "  \n",
    "  ## OUTPUT:        # returns the \"optimal\" Group Lasso and Sparsity penalty coeffients for each cvxpy solver along with the validation sets mean squared errors for \n",
    "                      each solver and lambda1 and lambda2\n",
    "  '''\n",
    "  if beg[0]!=0:\n",
    "    raise ValueError (\"beg should be a list like object with 0 as the first element\")\n",
    "\n",
    "  mse=dict()\n",
    "  opt_lambdas=dict()\n",
    "  n,p=X.shape\n",
    "\n",
    "  for solver in solvers:\n",
    "    mse[solver]=dict()\n",
    "    opt_lambdas[solver]=dict()\n",
    "    print(solver ,\":\")\n",
    "    for lamb1 in tqdm(lambdas1):\n",
    "      for lamb2 in lambdas2:\n",
    "        val_mse=[]\n",
    "        # running the optimization nrun times after permuting the data each time\n",
    "        for _ in range(nb_runs):\n",
    "          Xtrain,Xval,Ytrain,Yval=train_test_split(X,Y,train_threshold=0.8)\n",
    "          # solving the optimization problem\n",
    "          val_mse, _ = run_sparse_group_lasso(Xtrain, Ytrain, Xval, Yval, lamb1, lamb2, beg, val_mse, solver)\n",
    "        with warnings.catch_warnings():\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          mse[solver][(lamb1,lamb2)]=val_mse\n",
    "          # metric = standard deviation adjusted mean mse\n",
    "          opt_lambdas[solver][(lamb1,lamb2)]=np.mean(val_mse)+np.std(val_mse)\n",
    "\n",
    "    # optimal (lambda1, lambda2) and metric for a given solver\n",
    "    opt_lambdas[solver]=sorted(opt_lambdas[solver].items(), key=lambda item: item[1], reverse=False)[0]\n",
    "\n",
    "  # optimal solver = each solver is associated to its optimal adjusted mean mse\n",
    "  solver_opt_metric = {solver: opt_lambdas[solver][1] for solver in opt_lambdas} \n",
    "  opt_solver = sorted(solver_opt_metric.items(), key=lambda item: item[1], reverse=False)[0][0]\n",
    "\n",
    "  return mse, {opt_solver: opt_lambdas[opt_solver][0]}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr_saprse_group_lasso_one_simulation(exple, sim_number, lambdas1, lambdas2, train_threshold, solvers, eps=1e-6):\n",
    "    '''\n",
    "    ## INPUT:     # exple is a given example made of simulations\n",
    "                  # sim_number: number of the simulation\n",
    "                  # lambdas1 is a list of of sparsity penalty coefficient (L_1 norm)\n",
    "                  # lambdas2 is a list of Group Lasso penalty coefficient (L_2 norm)\n",
    "                  # train_threshold designates the proportion of data X,Y used for training\n",
    "                  # solvers is a list of solvers used to solve the group lasso optimization problem using cvxpy package\n",
    "                  # eps is the tolerance to agree on equality on two float numbers\n",
    "\n",
    "\n",
    "    ## OUTPUT:      # The True Positive and False Positive Rates\n",
    "    '''\n",
    "    # the data\n",
    "    beg=exple[\"beg\"]\n",
    "    beta_star=exple[\"beta\"]\n",
    "    X=exple[\"sim\"][sim_number][\"X\"]\n",
    "    Y=exple[\"sim\"][sim_number][\"Y\"]\n",
    "    Xtrain,Xval,Ytrain,Yval=train_test_split(X,Y,train_threshold=train_threshold)\n",
    "\n",
    "    # fine tuning lambda\n",
    "    mse, opt_lambda=optimal_sparse_group_lasso(Xtrain, Ytrain, lambdas1, lambdas2,\n",
    "                                               beg, train_threshold=train_threshold, nb_runs=nb_runs, solvers=solvers)\n",
    "    opt_solver, (lambda_star1, lambda_star2) = list(opt_lambda.items())[0]\n",
    "\n",
    "    # solving with the optimal lambda, on the whole dataset (without validating)\n",
    "    MSE, solution = run_sparse_group_lasso(Xtrain, Ytrain, Xval, Yval, lambda_star1, lambda_star2, beg, [], opt_solver)\n",
    "\n",
    "    TPR,FPR = np.nan, np.nan\n",
    "    if solution is not None:\n",
    "        # True Positive Rate = proportion of non-zero coefficients correctly detected \n",
    "        TP=np.where((abs(beta_star)>eps) & (abs(solution)>eps))[0]\n",
    "        # False positives: null coefficients that were estimated as non-zero\n",
    "        FP=np.where((abs(beta_star)<eps) & (abs(solution)>eps))[0]\n",
    "        # true zeros are the coordinates of zeros in the true solution\n",
    "        true_zeros=np.where(beta_star==0)[0]\n",
    "        # true non zeros are the non zeros coordinates of the true solution\n",
    "        true_non_zeros=np.where(beta_star!=0)[0] \n",
    "        TPR=len(TP)/len(true_non_zeros)\n",
    "        # FPR=FP/Negatives\n",
    "        FPR=len(FP)/len(true_zeros)\n",
    "    return((TPR,FPR,MSE[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n"
     ]
    }
   ],
   "source": [
    "lambdas1=[1e-6, 1e-4, 1e-2, 1, 10, 100, 1000, 1e4, 1e5, 1e6]\n",
    "lambdas2=[100]\n",
    "nb_runs = 50\n",
    "train_threshold=0.6\n",
    "solvers=[\"ECOS\"]\n",
    "\n",
    "results_tpr_fpr_mse = []\n",
    "for number in example:\n",
    "    print(f\"Example {number}\")\n",
    "    exple=example[number]\n",
    "    nb_sims=len(exple[\"sim\"])\n",
    "    results = Parallel(n_jobs=10)(delayed(tpr_fpr_saprse_group_lasso_one_simulation)\n",
    "        (exple, sim_number, lambdas1, lambdas2, train_threshold, solvers)\n",
    "        for sim_number in range(nb_sims))\n",
    "    TPRs = [x[0] for x in results if x!=np.nan]\n",
    "    FPRs = [x[1] for x in results if x!=np.nan]\n",
    "    MSEs = [x[2] for x in results if x!=np.nan]\n",
    "    results_tpr_fpr_mse.append([np.mean(TPRs), np.std(TPRs), np.mean(FPRs), np.std(FPRs), np.median(MSEs), np.std(MSEs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "\n",
      "    Mean TPR=0.8955555555555554, STD TPR=0.18245242911205342\n",
      "\n",
      "    Mean FPR=0.4218181818181818, STD FPR=0.3210313956995381\n",
      "\n",
      "    Median MSE=9.455899039139013, STD MSE=3.234905118261159\n",
      "\n",
      "Example 2:\n",
      "\n",
      "    Mean TPR=0.986, STD TPR=0.07213875518748572\n",
      "\n",
      "    Mean FPR=0.14657142857142857, STD FPR=0.09261771329679075\n",
      "\n",
      "    Median MSE=6.040333839231346, STD MSE=2.1089836048330275\n",
      "\n",
      "Example 3:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=0.593, STD FPR=0.3331681257263366\n",
      "\n",
      "    Median MSE=5.847779089221204, STD MSE=1.255514579810498\n",
      "\n",
      "Example 4:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=0.5026666666666667, STD FPR=0.2612652972661229\n",
      "\n",
      "    Median MSE=5.20056361437472, STD MSE=1.3368226804532515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for number in example:\n",
    "    results=results_tpr_fpr_mse[number-1]\n",
    "    print(f\"\"\"Example {number}:\\n\n",
    "    Mean TPR={results[0]}, STD TPR={results[1]}\\n\n",
    "    Mean FPR={results[2]}, STD FPR={results[3]}\\n\n",
    "    Median MSE={results[4]}, STD MSE={results[5]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woOMU9PftPBJ"
   },
   "source": [
    "## Bayesian Group Lasso with Spike and Slab Prior (BGL-SS)\n",
    "### First implementation\n",
    "This was our first implementation of this model. Unfortunately it didn't return correct results and we couldn't find the problem, so we coded another implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TQHV9gbc5IOi"
   },
   "outputs": [],
   "source": [
    "def get_except(data,g,beg):\n",
    "  '''\n",
    "  # INPUT:         # data is a dictionary of key in [\"X\", \"beta\"] if key=\"X\" then data should be the data of covariates\n",
    "                     otherwise it should be an np.array of the parameter beta\n",
    "                   # g is the number of the group to retrieve\n",
    "                   # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "\n",
    "  # OUTPUT:        # Returns X except the gth group , example get(X,0,beg) retrieves all X except the first group\n",
    "  '''\n",
    "  assert type(data)==dict\n",
    "  assert beg[0]==0 and len(beg)>=2\n",
    "  assert 0<=g<len(beg)\n",
    "  if list(data.keys())[0]==\"X\":\n",
    "    if g==0:\n",
    "      return data[\"X\"][:,beg[g+1]:]\n",
    "    elif g==len(beg)-1:\n",
    "      return data[\"X\"][:,:beg[g]]\n",
    "    else:\n",
    "      return np.concatenate([data[\"X\"][:, :beg[g]], data[\"X\"][:, beg[g+1]:]], axis=1)\n",
    "\n",
    "  elif list(data.keys())[0]==\"beta\":\n",
    "    if g==0:\n",
    "      return data[\"beta\"][beg[g+1]:]\n",
    "    elif g==len(beg)-1:\n",
    "      return data[\"beta\"][:beg[g]]\n",
    "    else:\n",
    "      return np.concatenate([data[\"beta\"][:beg[g]], data[\"beta\"][beg[g+1]:]])\n",
    "\n",
    "\n",
    "def Zgs_and_mgs(beta, beg, nb_covariates, eps=1e-6):\n",
    "  '''\n",
    "  # INPUT:      # beta is the parameter to estimate\n",
    "                # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                # nb_covariates is the total number of features/covariates\n",
    "\n",
    "  # OUTPUT:     # Returns Z_gs the indicators of the beta_g=0 and m_gs the lengths/dimensions of the beta_gs     \n",
    "  '''\n",
    "  Z_gs = []\n",
    "  m_gs = []\n",
    "  for g in range(len(beg)):\n",
    "    m_g = lgroup(g,beg,nb_covariates)\n",
    "    m_gs.append(m_g)\n",
    "    beta_g = get({\"beta\":beta}, g, beg)\n",
    "    Z_g = max((abs(beta_g)>eps)*1)\n",
    "    Z_gs.append(Z_g)\n",
    "  return np.array(Z_gs), np.array(m_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM8SebR9bdg8"
   },
   "source": [
    "#### Priors simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fvXkbOy_bhqR"
   },
   "outputs": [],
   "source": [
    "def BGLSSPriorSigmaSquared(alpha, gamma):\n",
    "  '''\n",
    "  # INPUT:      # alpha is the shape of the inverse gamma prior for sigma^2\n",
    "                # gamma is the scale of the inverse gamma prior for sigma^2\n",
    "  \n",
    "  # OUTPUT:     # a realisation of Inverse Gamma(alpha, gamma)\n",
    "  '''\n",
    "  return scipy.stats.invgamma.rvs(alpha , scale = gamma) #1/gamma_distribution(shape=alpha, scale=1/gamma)\n",
    "\n",
    "\n",
    "def BGLSSPriorTauSquared(lamb, beg, nb_covariates):\n",
    "  '''\n",
    "  # INPUT:      # lamb is the group lasso penalty coefficient\n",
    "                # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                # nb_covariates is the total number of features/covariates\n",
    "\n",
    "  # OUTPUT:     # realisations of the tau_g^2 s (tau_g squared)\n",
    "  '''\n",
    "  taus_squared=[]\n",
    "  for g in range(len(beg)):\n",
    "    m_g = lgroup(g,beg,nb_covariates)\n",
    "    tau_g_squared = gamma_distribution(shape=(m_g+1)/2, scale=(lamb**2)/2)\n",
    "    taus_squared.append(tau_g_squared) \n",
    "  return np.array(taus_squared)\n",
    "\n",
    "\n",
    "def BGLSSPriorPi0(a,b):\n",
    "  '''\n",
    "  # INPUT:      # a and b are parameters of a beta distribution\n",
    "  \n",
    "  # OUTPUT:     # a realisation of Beta(a, b)\n",
    "  '''\n",
    "  \n",
    "  return scipy.stats.beta.rvs(a, b) #beta_distribution.rvs(a=a, b=b)\n",
    "\n",
    "\n",
    "def BGLSSPriorBeta(sigma_squared, taus_squared, pi0):\n",
    "  '''\n",
    "  # INPUT:      # sigma_squared is a parameter of the BGL SS model\n",
    "                # taus_squared includes the different values of tau_g's^2 (taus squared)\n",
    "                # pi0 is a parameter of the BGL SS model\n",
    "  \n",
    "  # OUPUT:      # realisations of the beta_gs\n",
    "  '''\n",
    "  beta_gs=[]\n",
    "  for g in range(len(beg)):\n",
    "    m_g=lgroup(g,beg,X.shape[1])\n",
    "    zeros_g=np.array([0]*m_g)\n",
    "    unif=uniform_distribution(0,1)\n",
    "    if unif <= pi0:\n",
    "      beta_g=zeros_g\n",
    "    else:\n",
    "      beta_g=multivariate_normal(zeros_g, sigma_squared * taus_squared[g] * np.eye(m_g))\n",
    "    beta_gs.append(beta_g)\n",
    "  return beta_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q80poT5l-RE0"
   },
   "source": [
    "#### Posteriors parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "duGYHQFz-qSt"
   },
   "outputs": [],
   "source": [
    "def BGLSSsigma(g, X, beg, tau_squared):\n",
    "  '''\n",
    "  # INPUT:       # g is the number of the group to retrieve\n",
    "                 # X is the covariates data\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # tau_squared is a parameter of the BGL SS model\n",
    "  \n",
    "  # OUTPUT:      # Sigma_g (scaled covariance of the gaussian part of the posterior distribution of beta_g)\n",
    "  '''\n",
    "  # assert tau_squared>0\n",
    "  X_g = get({\"X\":X},g,beg)\n",
    "  m_g = lgroup(g,beg, X.shape[1])\n",
    "  \n",
    "  return  np.linalg.inv(X_g.T@X_g + (1/tau_squared)*np.eye(m_g))\n",
    "\n",
    "def BGLSSmu(g, X, Y, beta, beg, tau_squared):\n",
    "  '''\n",
    "  # INPUT:       # g is the number of the group to retrieve\n",
    "                 # X is the covariates data\n",
    "                 # Y is the labels data\n",
    "                 # beta is the parameter to estimate\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # tau_squared is a parameter of the BGL SS model\n",
    "  \n",
    "  # OUTPUT:      # mu_g (expectation of the gaussian part of the posterior distribution of beta_g)\n",
    "  '''\n",
    "  X_g = get({\"X\":X},g,beg)\n",
    "  X__g = get_except({\"X\":X},g,beg)\n",
    "  beta__g = get_except({\"beta\":beta},g,beg)\n",
    "  Sigma_g = BGLSSsigma(g, X, beg, tau_squared)\n",
    "  return Sigma_g@X_g.T@(Y-X__g@beta__g)\n",
    "\n",
    "\n",
    "def BGLSSPosteriorproba0(g, X, Y, beta, beg, tau_squared, pi0, sigma_squared):\n",
    "  '''\n",
    "  # INPUT:       # g is the number of the group to retrieve\n",
    "                 # X is the covariates data\n",
    "                 # Y is the labels data\n",
    "                 # beta is the parameter to estimate\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # tau_squared is a parameter of the BGL SS model\n",
    "                 # pi0 is a parameter of the BGL SS model\n",
    "                 # sigma_squared is a parameter of the BGL SS model\n",
    "  \n",
    "  # OUTPUT:      # the conditional posterior probability that beta_g=0\n",
    "  '''\n",
    "  m_g = lgroup(g,beg, X.shape[1])\n",
    "  X_g = get({\"X\":X},g,beg)\n",
    "  X__g = get_except({\"X\":X},g,beg)\n",
    "  beta__g = get_except({\"beta\":beta},g,beg)\n",
    "  Sigma_g = BGLSSsigma(g, X, beg, tau_squared)\n",
    "  arg_exp= (1/(2*sigma_squared))*(np.linalg.norm(sqrtm(Sigma_g)@X_g.T@(Y-X__g@beta__g),2)**2)\n",
    "  # we add a maximum value for the argument (arg_exp) in the exponential to avoid avoerflow problems\n",
    "  denominator=pi0+(1-pi0)*((tau_squared)**(-m_g/2))*np.sqrt(np.linalg.det(Sigma_g))*np.exp((min(arg_exp,500)))\n",
    "  # denominator=pi0+(1-pi0)*((tau_squared)**(-m_g/2))*np.sqrt(np.linalg.det(Sigma_g))*np.exp(arg_exp)\n",
    "\n",
    "  return pi0/denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbnKKHVZ-IJk"
   },
   "source": [
    "#### Posteriors simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1LtFbW5OVIpX"
   },
   "outputs": [],
   "source": [
    "def BGLSSPosteriorSigmaSquared(X,Y, beta, beg, taus_squared, alpha, gamma, eps):\n",
    "  '''\n",
    "  # INPUT:       # X is the covariates data\n",
    "                 # Y is the labels data\n",
    "                 # beta is the parameter to estimate\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # taus_squared includes the different values of tau's^2 (taus squared)\n",
    "                 # alpha is the shape of the inverse gamma prior for sigma^2\n",
    "                 # gamma is the scale of the inverse gamma prior for sigma^2\n",
    "                 # eps is the tolerance to agree on equality on two float numbers\n",
    "\n",
    "  # OUTPUT:      # a posterior random variable for sigma^2 (sigma squared)\n",
    "  '''\n",
    "  n,p = X.shape\n",
    "\n",
    "  Z_gs, m_gs = Zgs_and_mgs(beta, beg, p, eps=eps)\n",
    "\n",
    "  D_tau = []\n",
    "  for g in range(len(beg)):\n",
    "    D_tau.extend([taus_squared[g]]*m_gs[g])\n",
    "    \n",
    "  D_tau = np.diag(D_tau)\n",
    "\n",
    "  # parameters of the posterior inverse gamma\n",
    "  shape= (1/2)*(n+Z_gs@m_gs) + alpha\n",
    "  scale= (1/2)*( (Y-X@beta).T@(Y-X@beta) + beta.T@np.linalg.inv(D_tau)@beta ) + gamma\n",
    "  \n",
    "  return scipy.stats.invgamma.rvs(shape , scale = scale)#1/gamma_distribution(shape=shape, scale=1/scale) # not sure about this sampler, we want to sample from Inverse Gamma(shape, scale) ~ 1/Gamma(shape,1/scale) ?\n",
    "\n",
    "\n",
    "def BGLSSPosteriorPi0(beta, beg, a, b, nb_covariates, eps):\n",
    "  '''\n",
    "  # INPUT:       # beta is the parameter to estimate\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # a the first parameter of the Beta prior for pi0\n",
    "                 # b the second parameter of the Beta prior for pi0\n",
    "                 # nb_covariates is the total number of features/covariates\n",
    "                 # eps is the tolerance to agree on equality on two float numbers\n",
    "\n",
    "\n",
    "  # OUTPUT:      # a posterior random variable for pi0\n",
    "  '''  \n",
    "  Z_gs, m_gs = Zgs_and_mgs(beta, beg, nb_covariates, eps=eps)\n",
    "\n",
    "  # parameters of the posterior gamma\n",
    "  a_post = a + sum(Z_gs)\n",
    "  b_post = b + sum(m_gs-Z_gs) \n",
    "\n",
    "  return scipy.stats.beta.rvs(a_post, b_post)#beta_distribution.rvs(a=a_post, b=b_post)\n",
    "\n",
    "\n",
    "def BGLSSPosteriorTauSquared(beta, beg, lamb, sigma_squared, nb_covariates, n_tau_samples, eps):\n",
    "  '''\n",
    "  # INPUT:       # beta is the parameter to estimate\n",
    "                 # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                 # lamb is the group lasso penalty coefficient\n",
    "                 # sigma_squared is a parameter of the BGL SS model\n",
    "                 # nb_covariates is the total number of features/covariates\n",
    "                 # n_tau_samples is the number of samples for each tau_g^2\n",
    "                 # eps is the tolerance to agree on equality on two float numbers\n",
    "\n",
    "\n",
    "  # OUTPUT:      # a posterior random variable for the tau_g^2 (tau_g squared), many times\n",
    "  '''\n",
    "  assert n_tau_samples>=1\n",
    "  Z_gs, m_gs=Zgs_and_mgs(beta, beg, nb_covariates, eps)\n",
    "  taus_squared = []\n",
    "  for g in range(len(beg)):\n",
    "    beta_g = get({\"beta\":beta}, g, beg)\n",
    "    if Z_gs[g]==1:\n",
    "      # shape=lamb*np.sqrt(sigma_squared)/np.linalg.norm(beta_g,2) \n",
    "      # scale=lamb**2\n",
    "      shape=3/2 \n",
    "      scale=lamb**2/2 + np.linalg.norm(beta_g,2)/(2*sigma_squared**2) \n",
    "      tau_squared = gamma_distribution(shape=shape, scale=1/scale, size=n_tau_samples)\n",
    "    elif Z_gs[g]==0:\n",
    "      # shape=(m_gs[g]+1)/2\n",
    "      # scale=lamb**2/2\n",
    "      shape=(m_gs[g]+1)/2 + 2\n",
    "      scale=lamb**2/2      \n",
    "      tau_squared = gamma_distribution(shape=shape, scale=1/scale, size=n_tau_samples)\n",
    "    taus_squared.append(tau_squared)\n",
    "  \n",
    "  if n_tau_samples==1: return np.array(taus_squared).flatten() #.reshape(1,-1)[0]\n",
    "  return np.array(taus_squared) # each column represents a realization of the vector of the taug_s\n",
    "\n",
    "\n",
    "def BGLSSPosteriorBeta(X,Y, beta, beg, taus_squared, sigma_squared, pi0):\n",
    "  '''\n",
    "  # INPUT:      # X is the covariates data\n",
    "                # Y is the labels data\n",
    "                # beta is the parameter to estimate\n",
    "                # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                    beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                # taus_squared includes the different values of tau's^2 (taus squared)\n",
    "                # sigma_squared is a parameter of the BGL SS model\n",
    "  \n",
    "  # OUTPUT:     # The posterior beta_gs\n",
    "  '''\n",
    "  beta_gs=[]\n",
    "  for g in range(len(beg)):\n",
    "    tau_squared=taus_squared[g]\n",
    "    post0=BGLSSPosteriorproba0(g, X, Y, beta, beg, tau_squared, pi0, sigma_squared)\n",
    "    mu_g=BGLSSmu(g, X, Y, beta, beg, tau_squared)\n",
    "    Sigma_g=BGLSSsigma(g, X, beg, tau_squared)\n",
    "    # sampling beta_g\n",
    "    unif=uniform_distribution(0,1)\n",
    "    if unif <= post0:\n",
    "      m_g=lgroup(g,beg,X.shape[1])\n",
    "      beta_g=np.array([0]*m_g)\n",
    "    else:\n",
    "      beta_g=multivariate_normal(mu_g, sigma_squared * Sigma_g)\n",
    "    beta_gs.append(beta_g)\n",
    "\n",
    "  return beta_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2_Q3g9SEXOQH"
   },
   "outputs": [],
   "source": [
    "def BGLSSGibbsSampler(X, Y, beg, arg_prior, n_tau_samples, eps=1e-5, collect=False, nb_iter_max=100):\n",
    "  '''\n",
    "  # INPUT:          # arg_prior stores the priors parameters it is like {\"a\":..., \"b\":..., ...}\n",
    "                    # beg is the indices (starting with zero) of the beginning of the groups: example if there are 10 covariates and G groups, \n",
    "                      beg=[i_1, i_2,...i_G] of length G means that the first group starts at index i_1 (with i_1=0), the second group at i_2, etc.\n",
    "                    # n_tau_samples is the number of samples for each tau_g^2\n",
    "                    # collect tells whether or not to store the trajectory of the iterates\n",
    "                    # nb_iter_max is the maximal number of iterations \n",
    "  \n",
    "  # OUTPUT:         # The posterior estimate of the beta_g's\n",
    "  '''\n",
    "  assert n_tau_samples>=1\n",
    "  iterates = {\"beta_gs\": dict(), \"pi0\": dict(), \"sigma2\":dict(), \"tau2\": dict()}\n",
    "  p,G = X.shape[1], len(beg)\n",
    "  a, b, alpha, gamma, lamb = arg_prior[\"a\"], arg_prior[\"b\"], arg_prior[\"alpha\"], arg_prior[\"gamma\"], arg_prior[\"lamb\"]\n",
    "\n",
    "  sigma_squared = BGLSSPriorSigmaSquared(alpha, gamma)\n",
    "  taus_squared = BGLSSPriorTauSquared(lamb, beg, p)\n",
    "  pi0 = BGLSSPriorPi0(a,b)\n",
    "  beta_gs = BGLSSPriorBeta(sigma_squared, taus_squared, pi0)\n",
    "  \n",
    "  # print(sigma_squared)\n",
    "  for iter in tqdm(range(nb_iter_max)):\n",
    "  # for iter in range(nb_iter_max):\n",
    "    # making sure that updates are done on the previous values\n",
    "    beta = np.concatenate(beta_gs)\n",
    "    beta_gs, pi0, sigma_squared, taus_squared = BGLSSPosteriorBeta(X,Y, beta, beg, taus_squared, sigma_squared, pi0),\\\n",
    "                                                BGLSSPosteriorPi0(beta, beg, a, b, p, eps), \\\n",
    "                                                BGLSSPosteriorSigmaSquared(X,Y, beta, beg, taus_squared, alpha, gamma, eps),\\\n",
    "                                                BGLSSPosteriorTauSquared(beta, beg, lamb, sigma_squared, p, n_tau_samples, eps)\n",
    "    # using EM updates for lambda\n",
    "    if n_tau_samples==1:\n",
    "      # we recommend to set n_tau_samples greater than 1 to get more precise EM updates \n",
    "      average_cond_taus_squared=taus_squared\n",
    "    else: \n",
    "      average_cond_taus_squared=np.mean(taus_squared, axis=1)\n",
    "      taus_squared = taus_squared[:,0] # we could choose any other column here instead of the first,\n",
    "                                       # since each column is a realisation of the vector of tau_gs^2\n",
    "    lamb = np.sqrt((p+G)/sum(average_cond_taus_squared))\n",
    "    # print(lamb)\n",
    "    # print(sigma_squared, taus_squared, pi0)\n",
    "    # print(taus_squared)\n",
    "    # print(np.concatenate(results[\"beta_gs\"][nb_iter_max-1]))\n",
    "    # print(pi0)\n",
    "    if collect:\n",
    "      iterates[\"beta_gs\"][iter]=beta_gs\n",
    "      iterates[\"pi0\"][iter]=pi0\n",
    "      iterates[\"sigma2\"][iter]=sigma_squared\n",
    "      iterates[\"tau2\"][iter]=taus_squared\n",
    "\n",
    "  # collecting at least the last iterate of the beta_gs\n",
    "  iterates[\"beta_gs\"][iter]=beta_gs\n",
    "  \n",
    "  return iterates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vSP1lLMvxPaw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:54<00:00, 183.51it/s]\n"
     ]
    }
   ],
   "source": [
    "arg_prior = {\"a\":1, \"b\":1, \"alpha\":1, \"gamma\":0.01, \"lamb\":1}\n",
    "n_tau_samples = 50\n",
    "nb_iter_max=10000\n",
    "burn_in=5000\n",
    "number=1\n",
    "X=example[number][\"sim\"][0][\"X\"]\n",
    "Y=example[number][\"sim\"][0][\"Y\"]\n",
    "beta_star=example[number][\"beta\"]\n",
    "beg=example[number][\"beg\"]\n",
    "results=BGLSSGibbsSampler(X, Y, beg, arg_prior, 1, collect=True, nb_iter_max=nb_iter_max)\n",
    "# np.allclose(np.concatenate(results[\"beta_gs\"][nb_iter_max-1]),beta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Xv8bOjp-v3W-"
   },
   "outputs": [],
   "source": [
    "non_burn_in_betas=np.stack([np.concatenate(results[\"beta_gs\"][k]) for k in range(burn_in, nb_iter_max)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11991241e-07,  4.78783379e-08,  9.68388985e-08,  2.35499016e-07,\n",
       "        9.96378866e-08,  1.31047354e-07,  1.12941492e-07, -1.59620768e-08,\n",
       "       -1.19792678e-07,  4.26561100e-08, -6.61096469e-08, -1.08181316e-07,\n",
       "       -9.21999143e-08,  1.78353822e-07,  1.81409896e-07, -1.36577392e-07,\n",
       "       -1.20314160e-08, -2.06388880e-07, -1.58361369e-07,  7.31217447e-08])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(non_burn_in_betas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(non_burn_in_betas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3 , -1.  ,  0.  ,  0.5 ,  0.01,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.8 ,  0.8 ,  0.8 ,  0.8 ,  0.8 ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second implementation of BGL-SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossprod(x,y):\n",
    "    return x.T@y\n",
    "\n",
    "def BGL_EM_lambda(X,Y, beg, niter = 100, a=1, b=1, num_update = 100, niter_update =100,\n",
    "        alpha=0.1, gamma=0.1, pi=0.5, delta=0.001):\n",
    "# useful variables\n",
    "    n = len(Y)\n",
    "    p = X.shape[1]\n",
    "    group_size = np.diff(beg+[X.shape[1]])\n",
    "    ngroup = len(group_size)\n",
    " \n",
    "    # initializing parameters\n",
    "    tau2 = np.ones([ngroup])\n",
    "    sigma2 = 1\n",
    "    lambda2 = 1\n",
    "    matlambda2 = np.ones([ngroup])\n",
    "    lambda2_path = -1*np.ones([num_update])\n",
    "    matlambda2_path = -1*np.ones([num_update, ngroup])\n",
    "    l = np.zeros([ngroup])\n",
    "    beta = []\n",
    "    for i in range(ngroup):\n",
    "        beta.append(np.zeros([group_size[i]]))\n",
    "    Z = np.zeros([ngroup])\n",
    " \n",
    "    # reusable values are computed here to avoid recomputing them every iteration\n",
    "    YtY = Y.T@Y\n",
    "    XtY = X.T@Y\n",
    "    XtX = X.T@X\n",
    " \n",
    "    # group\n",
    "    XktY = [0]*ngroup\n",
    "    XktXk = [0]*ngroup\n",
    "    XktXmk = [0]*ngroup\n",
    " \n",
    "    begin_idx = 0\n",
    " \n",
    "    for i in range(ngroup):\n",
    "        end_idx = begin_idx + group_size[i] - 1\n",
    "        Xk = X[:,begin_idx:(end_idx+1)]\n",
    "        XktY[i] = crossprod(Xk, Y)\n",
    "        XktXk[i] = crossprod(Xk, Xk)\n",
    "        XktXmk[i] = crossprod(Xk, np.delete(X, np.arange(begin_idx,end_idx+1), axis=1))\n",
    "        begin_idx = end_idx + 1\n",
    " \n",
    "    ##### Gibbs sampler\n",
    " \n",
    "    for update in range(num_update):\n",
    "        # initialize coefficients vector\n",
    "        coef = np.zeros((p, niter))\n",
    "        tau2_each_update = np.zeros((ngroup, niter))\n",
    " \n",
    "        for iteration in range(niter):\n",
    "            for i in range(ngroup):\n",
    "                bmk = np.array([])\n",
    "                for j in range(ngroup):\n",
    "                    if(j!=i):\n",
    "                        bmk = np.concatenate((bmk, beta[j]))\n",
    "                f1 = XktY[i] - XktXmk[i] @ bmk\n",
    "                f2 = XktXk[i]+1/tau2[i]*np.eye(group_size[i])\n",
    "                f2_inverse = np.linalg.inv(f2)\n",
    "                mu = f2_inverse @ f1\n",
    " \n",
    "                l[i] = pi/(pi+(1-pi)*(tau2[i])**(-group_size[i]/2)*np.linalg.det(f2)**(-1/2)*np.exp(f1.T@mu/(2*sigma2)))\n",
    "                maxf = np.max(f2)\n",
    "                li = (-group_size[i]/2)*np.log(tau2[i]) + (-1/2)*np.log(np.linalg.det(f2/maxf)) -f2.shape[0]/2*np.log(maxf) + f1@mu/(2*sigma2)\n",
    "                l[i] = pi/(pi+(1-pi)*np.exp(li))\n",
    " \n",
    "                if np.random.uniform()<l[i]:\n",
    "                    beta[i] = np.zeros([group_size[i]])\n",
    "                    Z[i] = 0\n",
    "                else:\n",
    "                    beta[i] = np.random.multivariate_normal(mean=mu, cov=sigma2*f2_inverse)\n",
    "                    Z[i] = 1\n",
    " \n",
    "            # Update tau2's\n",
    "            for i in range(ngroup):\n",
    "                if (Z[i]==0):\n",
    "                    tau2[i] = scipy.stats.gamma.rvs(a=(group_size[i]+1)/2, scale = 2/matlambda2[i])\n",
    "                else:\n",
    "                    scale = matlambda2[i]\n",
    "                    tau2[i] = 1/scipy.stats.invgauss.rvs(mu=1/scale*np.sqrt(matlambda2[i]*sigma2/sum(beta[i]**2)),\n",
    "                                                         scale=scale)\n",
    "            tau2_each_update[:,iteration] = tau2\n",
    "            \n",
    "            # Update sigma2\n",
    "            s=0\n",
    "            for i in range(ngroup):\n",
    "                s += np.sum(beta[i]**2)/tau2[i]\n",
    "            beta_vec = np.array([])\n",
    "            for j in range(ngroup):\n",
    "                beta_vec = np.concatenate((beta_vec, beta[j]))\n",
    "            coef[:,iteration] = beta_vec\n",
    "            sigma2 = scipy.stats.invgamma.rvs((n-1)/2 + np.sum(Z*group_size)/2 + alpha,\n",
    "                                              scale=(YtY-2*beta_vec@XtY+beta_vec@XtX@beta_vec+s)/2 + gamma)\n",
    " \n",
    "            # Update pi\n",
    "            pi = scipy.stats.beta.rvs(a+ngroup-sum(Z), b+sum(Z))\n",
    "            \n",
    "        # Update lambda\n",
    "        tau2_mean = np.mean(tau2_each_update, axis=1)\n",
    " \n",
    "        lambda2 = (p + ngroup) / np.sum(tau2_mean*group_size)\n",
    " \n",
    "        matlambda2 = (p + ngroup) / np.sum(tau2_mean)*np.ones([ngroup])\n",
    "        matlambda2_path[update,:] = matlambda2\n",
    " \n",
    "    return(matlambda2_path[-1])\n",
    " \n",
    "def BGL_SS(X,Y, beg, niter = 10000, burnin = 5000, a=1, b=1, num_update = 100, niter_update =100,\n",
    "        alpha=1e-1, gamma=1e-1, pi=0.5):\n",
    "    # useful variables\n",
    "    n = len(Y)\n",
    "    p = X.shape[1]\n",
    "    group_size = np.diff(beg+[X.shape[1]])\n",
    "    ngroup = len(group_size)\n",
    " \n",
    "    # initializing parameters\n",
    "    tau2 = np.ones([ngroup])\n",
    "    sigma2 = 1\n",
    "    l = np.zeros([ngroup])\n",
    "    beta = []\n",
    "    for i in range(ngroup):\n",
    "        beta.append(np.zeros([group_size[i]]))\n",
    "    Z = np.zeros([ngroup])\n",
    " \n",
    "    # EM Monte-Carlo sampling for lambda\n",
    "    lambda2 = BGL_EM_lambda(X, Y, beg, num_update = num_update, niter = niter_update)\n",
    " \n",
    "    # reusable values are computed here to avoid recomputing them every iteration\n",
    "    YtY = Y.T@Y\n",
    "    XtY = X.T@Y\n",
    "    XtX = X.T@X\n",
    " \n",
    "    # group\n",
    "    XktY = [0]*ngroup\n",
    "    XktXk = [0]*ngroup\n",
    "    XktXmk = [0]*ngroup\n",
    " \n",
    "    begin_idx = 0\n",
    " \n",
    "    for i in range(ngroup):\n",
    "        end_idx = begin_idx + group_size[i] - 1\n",
    "        Xk = X[:,begin_idx:(end_idx+1)]\n",
    "        XktY[i] = crossprod(Xk, Y)\n",
    "        XktXk[i] = crossprod(Xk, Xk)\n",
    "        XktXmk[i] = crossprod(Xk, np.delete(X, np.arange(begin_idx,end_idx+1), axis=1))\n",
    "        begin_idx = end_idx + 1\n",
    " \n",
    "    ##### Gibbs sampler\n",
    " \n",
    "    # initialize coefficients vector\n",
    "    coef = np.zeros((p, niter-burnin))\n",
    "    coef_tau = np.zeros((ngroup, niter))\n",
    " \n",
    "    for iteration in range(niter):\n",
    "        for i in range(ngroup):\n",
    "            bmk = np.array([])\n",
    "            for j in range(ngroup):\n",
    "                if(j!=i):\n",
    "                    bmk = np.concatenate((bmk, beta[j]))\n",
    "            f1 = XktY[i] - XktXmk[i] @ bmk\n",
    "            f2 = XktXk[i]+1/tau2[i]*np.eye(group_size[i])\n",
    "            f2_inverse = np.linalg.inv(f2)\n",
    "            mu = f2_inverse @ f1\n",
    " \n",
    "            l[i] = pi/(pi+(1-pi)*(tau2[i])**(-group_size[i]/2)*np.linalg.det(f2)**(-1/2)*np.exp(f1.T@mu/(2*sigma2)))\n",
    "            maxf = np.max(f2)\n",
    "            li = (-group_size[i]/2)*np.log(tau2[i]) + (-1/2)*np.log(np.linalg.det(f2/maxf)) -f2.shape[0]/2*np.log(maxf) + f1@mu/(2*sigma2)\n",
    "            l[i] = pi/(pi+(1-pi)*np.exp(li))\n",
    " \n",
    "            if np.random.uniform()<l[i]:\n",
    "                beta[i] = np.zeros([group_size[i]])\n",
    "                Z[i] = 0\n",
    "            else:\n",
    "                beta[i] = np.random.multivariate_normal(mean=mu, cov=sigma2*f2_inverse)\n",
    "                Z[i] = 1\n",
    " \n",
    "        # Update tau2's\n",
    "        for i in range(ngroup):\n",
    "            if (Z[i]==0):\n",
    "                tau2[i] = scipy.stats.gamma.rvs(a=(group_size[i]+1)/2, scale = 2/lambda2[i])\n",
    "            else:\n",
    "                scale = lambda2[i]\n",
    "                tau2[i] = 1/scipy.stats.invgauss.rvs(mu=1/scale*np.sqrt(lambda2[i]*sigma2/sum(beta[i]**2)), scale=scale)\n",
    " \n",
    " \n",
    "        # Update sigma2\n",
    "        s=0\n",
    "        for i in range(ngroup):\n",
    "            s += np.sum(beta[i]**2)/tau2[i]\n",
    "        beta_vec = np.array([])\n",
    "        for j in range(ngroup):\n",
    "            beta_vec = np.concatenate((beta_vec, beta[j]))\n",
    "        if(iteration > burnin):\n",
    "            coef[:,iteration-burnin] = beta_vec\n",
    "        sigma2 = scipy.stats.invgamma.rvs((n-1)/2 + np.sum(Z*group_size)/2 + alpha,\n",
    "                                          scale=(YtY-2*beta_vec@XtY+beta_vec@XtX@beta_vec+s)/2 + gamma)\n",
    " \n",
    "        # Update pi\n",
    "        pi = scipy.stats.beta.rvs(a+ngroup-sum(Z), b+sum(Z))\n",
    " \n",
    "    return(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31237424 -1.44312289  0.62976847  0.17568707  0.33811423 -0.15019826\n",
      " -0.44729331  0.5191342   0.3681237   0.30029637  0.52094698  1.12498854\n",
      "  0.23154992  1.30558058  0.51779142 -0.04420069 -0.04801424  0.07679263\n",
      "  0.00344509 -0.05656754]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "coef = BGL_SS(X,Y,beg) # 5000 posterior samples of beta values\n",
    "print(np.mean(coef, axis = 1)) # mean of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(exple, sim_number, beta_hat, beta_star, train_threshold=0.6, eps=1e-6):\n",
    "    # True Positive Rate = proportion of non-zero coefficients correctly detected \n",
    "    TP=np.where((abs(beta_star)>eps) & (abs(beta_hat)>eps))[0]\n",
    "    # False positives: null coefficients that were estimated as non-zero\n",
    "    FP=np.where((abs(beta_star)<eps) & (abs(beta_hat)>eps))[0]\n",
    "    # true zeros are the coordinates of zeros in the true solution\n",
    "    true_zeros=np.where(beta_star==0)[0]\n",
    "    # true non zeros are the non zeros coordinates of the true solution\n",
    "    true_non_zeros=np.where(beta_star!=0)[0] \n",
    "    TPR=len(TP)/len(true_non_zeros)\n",
    "    # FPR=FP/Negatives\n",
    "    FPR=len(FP)/len(true_zeros)\n",
    "    \n",
    "    Yval = exple[\"sim\"][sim_number]['Y']\n",
    "    Yval = Yval[int(train_threshold*len(Yval)):]\n",
    "    Xval = exple[\"sim\"][sim_number]['X']\n",
    "    Xval = Xval[int(train_threshold*len(Xval)):]\n",
    "    MSE = np.linalg.norm(Yval-Xval@beta_hat,2)**2/len(Yval)\n",
    "    return(TPR, FPR, MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BGL_SS_simulation(exple, sim_number, train_threshold):\n",
    "    X = exple[\"sim\"][sim_number]['X']\n",
    "    X = X[:int(train_threshold*len(X))]\n",
    "    Y = exple[\"sim\"][sim_number]['Y']\n",
    "    Y = Y[:int(train_threshold*len(Y))]\n",
    "    coef = BGL_SS(X, Y, exple[\"beg\"])\n",
    "    return([sim_number, coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n"
     ]
    }
   ],
   "source": [
    "nb_runs = 50\n",
    "train_threshold=0.6\n",
    "l_coefs = []\n",
    "\n",
    "results_tpr_fpr_mse = []\n",
    "for number in example:\n",
    "    print(f\"Example {number}\")\n",
    "    exple=example[number]\n",
    "    l_coefs.append( Parallel(n_jobs=10)(delayed(run_BGL_SS_simulation)\n",
    "        (exple, sim_number, train_threshold)\n",
    "        for sim_number in range(nb_runs))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of using the sample mean, median, or credible interval to select variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_med = []\n",
    "results_mean = []\n",
    "for number in example:\n",
    "    exple = example[number]\n",
    "    beta_star = exple['beta']\n",
    "    for run in range(nb_runs):\n",
    "        sim_number = l_coefs[number-1][run][0]\n",
    "        beta_median = np.median(l_coefs[number-1][run][1], axis=1)\n",
    "        beta_mean = np.mean(l_coefs[number-1][run][1], axis=1)\n",
    "        results_med.append(calc_metrics(exple, sim_number, beta_median,beta_star))\n",
    "        results_mean.append(calc_metrics(exple, sim_number, beta_mean,beta_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=0.5338452380952375, STD FPR=0.38322638832794964\n",
      "\n",
      "    Mean MSE=6.815056405653777, STD MSE=2.5798994262096056\n",
      "\n",
      "Median:\n",
      "\n",
      "    Mean TPR=0.8705555555555557, STD TPR=0.19269931160063397\n",
      "\n",
      "    Mean FPR=0.06954545454545456, STD FPR=0.1681148515324729\n",
      "\n",
      "    Mean MSE=6.97951086802077, STD MSE=2.697207299379015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_mean_all = np.mean(results_mean, axis=0)\n",
    "results_mean_all_std = np.std(results_mean, axis=0)\n",
    "results_med_all = np.mean(results_med, axis=0)\n",
    "results_med_all_std = np.std(results_med, axis=0)\n",
    "\n",
    "print(f\"\"\"Mean:\\n\n",
    "    Mean TPR={results_mean_all[0]}, STD TPR={results_mean_all_std[0]}\\n\n",
    "    Mean FPR={results_mean_all[1]}, STD FPR={results_mean_all_std[1]}\\n\n",
    "    Mean MSE={results_mean_all[2]}, STD MSE={results_mean_all_std[2]}\\n\n",
    "Median:\\n\n",
    "    Mean TPR={results_med_all[0]}, STD TPR={results_med_all_std[0]}\\n\n",
    "    Mean FPR={results_med_all[1]}, STD FPR={results_med_all_std[1]}\\n\n",
    "    Mean MSE={results_med_all[2]}, STD MSE={results_med_all_std[2]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credibility interval 0.1:\n",
      "\n",
      "        Mean TPR=0.8453333333333333, STD TPR=0.20400423621842206\n",
      "\n",
      "        Mean FPR=0.05090909090909091, STD FPR=0.13162682034718728\n",
      "\n",
      "Credibility interval 0.2:\n",
      "\n",
      "        Mean TPR=0.8263888888888888, STD TPR=0.2175436312133304\n",
      "\n",
      "        Mean FPR=0.036818181818181805, STD FPR=0.10127187040603723\n",
      "\n",
      "Credibility interval 0.3:\n",
      "\n",
      "        Mean TPR=0.8091111111111111, STD TPR=0.22651890608691969\n",
      "\n",
      "        Mean FPR=0.02727272727272726, STD FPR=0.08479435502807962\n",
      "\n",
      "Credibility interval 0.4:\n",
      "\n",
      "        Mean TPR=0.7939999999999999, STD TPR=0.23910801319761074\n",
      "\n",
      "        Mean FPR=0.019090909090909082, STD FPR=0.06716822349842108\n",
      "\n",
      "Credibility interval 0.5:\n",
      "\n",
      "        Mean TPR=0.7709999999999999, STD TPR=0.25322314855273975\n",
      "\n",
      "        Mean FPR=0.011818181818181816, STD FPR=0.04921700975868761\n",
      "\n",
      "Credibility interval 0.6:\n",
      "\n",
      "        Mean TPR=0.7464444444444445, STD TPR=0.27699614473405804\n",
      "\n",
      "        Mean FPR=0.007272727272727272, STD FPR=0.03562894171320984\n",
      "\n",
      "Credibility interval 0.7:\n",
      "\n",
      "        Mean TPR=0.7167777777777777, STD TPR=0.3139768608286323\n",
      "\n",
      "        Mean FPR=0.005, STD FPR=0.02903047700333771\n",
      "\n",
      "Credibility interval 0.8:\n",
      "\n",
      "        Mean TPR=0.6907777777777778, STD TPR=0.33942418561112936\n",
      "\n",
      "        Mean FPR=0.002272727272727273, STD FPR=0.021198588757020156\n",
      "\n",
      "Credibility interval 0.9:\n",
      "\n",
      "        Mean TPR=0.6515, STD TPR=0.38354772989956737\n",
      "\n",
      "        Mean FPR=0.0009090909090909091, STD FPR=0.012824305436059967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cred_level in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    eps = 1e-6\n",
    "    results_cred = []\n",
    "    for number in example:\n",
    "        beta_star = example[number]['beta']\n",
    "        for run in range(nb_runs):\n",
    "            cred_lower = np.quantile(l_coefs[number-1][run][1], (1-cred_level)/2, axis=1)\n",
    "            cred_upper = np.quantile(l_coefs[number-1][run][1], (1+cred_level)/2, axis=1)\n",
    "            zero_in_cred = ((cred_lower<=0) & (cred_upper>=0))\n",
    "            \n",
    "            # True Positive Rate = proportion of non-zero coefficients correctly detected \n",
    "            TP=np.where((abs(beta_star)>eps) & (np.logical_not(zero_in_cred)))[0]\n",
    "            # False positives: null coefficients that were estimated as non-zero\n",
    "            FP=np.where((abs(beta_star)<eps) & (np.logical_not(zero_in_cred)))[0]\n",
    "            # true zeros are the coordinates of zeros in the true solution\n",
    "            true_zeros=np.where(beta_star==0)[0]\n",
    "            # true non zeros are the non zeros coordinates of the true solution\n",
    "            true_non_zeros=np.where(beta_star!=0)[0] \n",
    "            TPR=len(TP)/len(true_non_zeros)\n",
    "            # FPR=FP/Negatives\n",
    "            FPR=len(FP)/len(true_zeros)\n",
    "\n",
    "            results_cred.append((TPR,FPR))\n",
    "\n",
    "    results_cred_all = np.mean(results_cred, axis=0)\n",
    "    results_cred_all_std = np.std(results_cred, axis=0)\n",
    "    print(f\"\"\"Credibility interval {cred_level}:\\n\n",
    "        Mean TPR={results_cred_all[0]}, STD TPR={results_cred_all_std[0]}\\n\n",
    "        Mean FPR={results_cred_all[1]}, STD FPR={results_cred_all_std[1]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median works better than the mean (slightly lower TPR, much lower FPR). Credible intervals also allow us to achieve very low FPRs while still retaining a good TPR, which is very good when parcimony is required and we want to avoid false positives.\n",
    "\n",
    "Now we will use the median since it provides a (subjectively) good compromise between TPR and FPR and we'll calculate the results for each example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_med_example = [[],[],[],[]]\n",
    "for number in example:\n",
    "    exple = example[number]\n",
    "    beta_star = exple['beta']\n",
    "    for run in range(nb_runs):\n",
    "        sim_number = l_coefs[number-1][run][0]\n",
    "        beta_median = np.median(l_coefs[number-1][run][1], axis=1)\n",
    "        results_med_example[number-1].append(calc_metrics(exple, sim_number, beta_median,beta_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGL-SS (Median):\n",
      "Example 1:\n",
      "\n",
      "        Mean TPR=0.7822222222222224, STD TPR=0.16771522012545068\n",
      "\n",
      "        Mean FPR=0.27818181818181825, STD FPR=0.23454545454545453\n",
      "\n",
      "        Median MSE=9.981128205069068, STD MSE=2.2127219559456472\n",
      "\n",
      "Example 2:\n",
      "\n",
      "        Mean TPR=0.6999999999999998, STD TPR=0.22360679774997907\n",
      "\n",
      "        Mean FPR=0.0, STD FPR=0.0\n",
      "\n",
      "        Median MSE=6.7768455167829575, STD MSE=2.53196896104015\n",
      "\n",
      "Example 3:\n",
      "\n",
      "        Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "        Mean FPR=0.0, STD FPR=0.0\n",
      "\n",
      "        Median MSE=5.919178364960111, STD MSE=1.4343378042472892\n",
      "\n",
      "Example 4:\n",
      "\n",
      "        Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "        Mean FPR=0.0, STD FPR=0.0\n",
      "\n",
      "        Median MSE=4.9008622709400775, STD MSE=0.9178298177633211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"BGL-SS (Median):\"\"\")\n",
    "for number in range(4):\n",
    "    results_med_all = np.mean(results_med_example[number], axis=0)\n",
    "    results_med_all_mse = np.median(results_med_example[number], axis=0)\n",
    "    results_med_all_std = np.std(results_med_example[number], axis=0)\n",
    "\n",
    "    print(f\"\"\"Example {number+1}:\\n\n",
    "        Mean TPR={results_med_all[0]}, STD TPR={results_med_all_std[0]}\\n\n",
    "        Mean FPR={results_med_all[1]}, STD FPR={results_med_all_std[1]}\\n\n",
    "        Median MSE={results_med_all_mse[2]}, STD MSE={results_med_all_std[2]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Sparse Group Selection with Spike and Slab priors (BSGS-SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BSGSSS_EM_t(X, Y, beg, niter = 100, num_update = 100, pi0 = 0.5, pi1 = 0.5,\n",
    "                       alpha = 1e-1, gamma = 1e-1, a1 = 1, a2 = 1,\n",
    "                       c1 = 1, c2 = 1, pi_prior = True, t = 1):\n",
    "    # book keeping\n",
    "    n = len(Y)\n",
    "    m = X.shape[1]\n",
    "    group_size = np.diff(beg+[X.shape[1]])\n",
    "    ngroup = len(beg)\n",
    "\n",
    "    # initializing parameters\n",
    "    b = np.ones([m])\n",
    "    b_prob = 0.5*np.ones([ngroup])\n",
    "    tau = np.ones([m])\n",
    "    tau_prob = 0.5*np.ones([m])\n",
    "    beta = np.ones([m])\n",
    "    sigma2 = 1\n",
    "    s2 = 1\n",
    "    \n",
    "    # EM MC for t, should be added here\n",
    "    # for example 1 it should be around 0.65\n",
    "    t = 0.65\n",
    "\n",
    "    # reusable values are computed here to avoid recomputing them every iteration\n",
    "    YtY = Y.T@Y\n",
    "    XtY = X.T@Y\n",
    "    XtX = X.T@X\n",
    "\n",
    "    # group\n",
    "    XktY = [0]*ngroup\n",
    "    XktXk = [0]*ngroup\n",
    "    XktXmk = [0]*ngroup\n",
    "\n",
    "    begin_idx = 0\n",
    "\n",
    "    for i in range(ngroup):\n",
    "        end_idx = begin_idx + group_size[i] - 1\n",
    "        Xk = X[:,begin_idx:(end_idx+1)]\n",
    "        XktY[i] = crossprod(Xk, Y)\n",
    "        XktXk[i] = crossprod(Xk, Xk)\n",
    "        XktXmk[i] = crossprod(Xk, np.delete(X, np.arange(begin_idx,end_idx+1), axis=1))\n",
    "        begin_idx = end_idx + 1\n",
    "\n",
    "    # individual\n",
    "    YtXi = np.zeros([m])\n",
    "    XmitXi = np.zeros((m,m-1))\n",
    "    XitXi = np.zeros([m])\n",
    "    for j in range(m):\n",
    "        YtXi[j] = crossprod(Y, X[:,j])\n",
    "        XmitXi[j,:] = crossprod(np.delete(X,j, axis=1), X[:,j])\n",
    "        XitXi[j] = crossprod(X[:,j], X[:,j])\n",
    "\n",
    "    ##### Gibbs sampler\n",
    "\n",
    "    t_path = -1*np.ones([num_update])\n",
    "    \n",
    "    for update in range(num_update):\n",
    "        # initialize coefficients vector\n",
    "        coef = np.zeros((m, niter))\n",
    "        bprob = -1*np.ones((ngroup, niter))\n",
    "        tauprob = -1*np.zeros((m, niter))\n",
    "        s2_vec = -1*np.ones([niter])\n",
    "        \n",
    "        for iteration in range(niter):\n",
    "            # number of non zero groups\n",
    "            n_nzg = 0\n",
    "            # number of non zero taus\n",
    "            n_nzt = 0\n",
    "\n",
    "            ### Generate tau ###\n",
    "            unif_samples = np.random.uniform(size = [m])\n",
    "            idx = 0\n",
    "            for g in range(ngroup):\n",
    "                for j in range(group_size[g]):\n",
    "                    M = (YtXi[idx] * b[idx] - crossprod(np.delete(beta, idx, axis=0), XmitXi[idx,:]) * b[idx])/sigma2\n",
    "                    N = 1/s2 + XitXi[idx] * b[idx]**2 / sigma2\n",
    "                    tau_prob[idx] = pi1/( pi1 + 2 * (1-pi1) * (s2*N)**(-0.5)* np.exp(scipy.stats.norm.logcdf(M/np.sqrt(N)) + M**2/(2*N)) )\n",
    "                    if (unif_samples[idx] < tau_prob[idx]):\n",
    "                        tau[idx] = 0\n",
    "                    else:\n",
    "                        tau[idx] = scipy.stats.truncnorm.rvs(a=-M/N/np.sqrt(1/N), b=np.inf, loc=M/N, scale=np.sqrt(1/N))\n",
    "                        n_nzt = n_nzt + 1\n",
    "                    idx = idx + 1\n",
    "\n",
    "            # generate b and compute beta\n",
    "            begin_idx = 0\n",
    "            unif_samples_group = np.random.uniform(size=ngroup)\n",
    "\n",
    "            for g in range(ngroup):\n",
    "                end_idx = begin_idx + group_size[g]\n",
    "                # Covariance Matrix for Group g\n",
    "                Vg = np.diag(tau[begin_idx:end_idx])\n",
    "                Sig = np.linalg.inv(crossprod(Vg, XktXk[g]) @ Vg / sigma2 + np.eye(group_size[g]))\n",
    "                dev = (Vg @ (XktY[g] - XktXmk[g] @ np.delete(beta,np.arange(begin_idx,end_idx), axis=0)))/sigma2\n",
    "                # probability for bg to be 0 vector\n",
    "                b_prob[g] = pi0 / ( pi0 + (1-pi0) * np.linalg.det(Sig)**(0.5) * np.exp(crossprod(dev,Sig)@dev/2) )\n",
    "                if (unif_samples_group[g] < b_prob[g]):\n",
    "                    b[begin_idx:end_idx] = 0\n",
    "                else:\n",
    "                    b[begin_idx:end_idx] = np.random.multivariate_normal(mean=Sig@dev, cov=Sig)\n",
    "                    n_nzg = n_nzg + 1\n",
    "                # Compute beta\n",
    "                beta[begin_idx:end_idx] = Vg @ b[begin_idx:end_idx]\n",
    "                # Update index\n",
    "                begin_idx = end_idx\n",
    "\n",
    "            # store coefficients vector beta\n",
    "            coef[:,iteration] = beta\n",
    "            bprob[:,iteration] = b_prob\n",
    "            tauprob[:,iteration] = tau_prob\n",
    "\n",
    "            # Generate sigma2\n",
    "            shape = n/2 + alpha\n",
    "            scale = (YtY + crossprod(beta,XtX)@beta - 2*crossprod(beta, XtY))/2 + gamma\n",
    "            sigma2 = scipy.stats.invgamma.rvs(shape, scale=scale)\n",
    "\n",
    "            if pi_prior:\n",
    "                # Generate pi0\n",
    "                pi0 = scipy.stats.beta.rvs(ngroup - n_nzg + a1, n_nzg + a2)\n",
    "                # Generate pi1\n",
    "                pi1 = scipy.stats.beta.rvs(m - n_nzt + c1, n_nzt + c2)\n",
    "\n",
    "            # Update s2\n",
    "            s2 = scipy.stats.invgamma.rvs(1 + n_nzt/2, scale = t + sum(tau)/2)\n",
    "            s2_vec[iteration] = s2\n",
    "        # Update t\n",
    "        t = 1 / (np.mean(1/s2_vec))\n",
    "        t_path[update] = t\n",
    "    return(t_path[-1])\n",
    "\n",
    "\n",
    "def BSGS_SS(X,Y,beg):\n",
    "    # default parameters\n",
    "    group_size = np.diff(beg+[X.shape[1]])\n",
    "    niter = 10000\n",
    "    burnin = 5000\n",
    "    pi0 = 0.5\n",
    "    pi1 = 0.5\n",
    "    num_update = 100\n",
    "    niter_update = 100\n",
    "    alpha = 1e-1\n",
    "    gamma = 1e-1\n",
    "    a1 = 1\n",
    "    a2 = 1\n",
    "    c1 = 1\n",
    "    c2 = 1\n",
    "    pi_prior = True\n",
    "\n",
    "    # book keeping\n",
    "    n = len(Y)\n",
    "    m = X.shape[1]\n",
    "    ngroup = len(group_size)\n",
    "\n",
    "    # initializing parameters\n",
    "    b = np.ones([m])\n",
    "    b_prob = 0.5*np.ones([ngroup])\n",
    "    tau = np.ones([m])\n",
    "    tau_prob = 0.5*np.ones([m])\n",
    "    beta = np.ones([m])\n",
    "    sigma2 = 1\n",
    "    s2 = 1\n",
    "    \n",
    "    # EM MC for t, should be added here\n",
    "    # for example 1 it should be around 0.65\n",
    "    t = BSGSSS_EM_t(X,Y,beg)\n",
    "\n",
    "    # reusable values are computed here to avoid recomputing them every iteration\n",
    "    YtY = Y.T@Y\n",
    "    XtY = X.T@Y\n",
    "    XtX = X.T@X\n",
    "\n",
    "    # group\n",
    "    XktY = [0]*ngroup\n",
    "    XktXk = [0]*ngroup\n",
    "    XktXmk = [0]*ngroup\n",
    "\n",
    "    begin_idx = 0\n",
    "\n",
    "    for i in range(ngroup):\n",
    "        end_idx = begin_idx + group_size[i] - 1\n",
    "        Xk = X[:,begin_idx:(end_idx+1)]\n",
    "        XktY[i] = crossprod(Xk, Y)\n",
    "        XktXk[i] = crossprod(Xk, Xk)\n",
    "        XktXmk[i] = crossprod(Xk, np.delete(X, np.arange(begin_idx,end_idx+1), axis=1))\n",
    "        begin_idx = end_idx + 1\n",
    "\n",
    "    # individual\n",
    "    YtXi = np.zeros([m])\n",
    "    XmitXi = np.zeros((m,m-1))\n",
    "    XitXi = np.zeros([m])\n",
    "    for j in range(m):\n",
    "        YtXi[j] = crossprod(Y, X[:,j])\n",
    "        XmitXi[j,:] = crossprod(np.delete(X,j, axis=1), X[:,j])\n",
    "        XitXi[j] = crossprod(X[:,j], X[:,j])\n",
    "\n",
    "    ##### Gibbs sampler\n",
    "\n",
    "    # initialize coefficients vector\n",
    "    coef = np.zeros((m, niter-burnin))\n",
    "    bprob = -1*np.ones((ngroup, niter-burnin))\n",
    "    tauprob = -1*np.zeros((m, niter-burnin))\n",
    "\n",
    "    for iteration in range(niter):\n",
    "        # number of non zero groups\n",
    "        n_nzg = 0\n",
    "        # number of non zero taus\n",
    "        n_nzt = 0\n",
    "\n",
    "        ### Generate tau ###\n",
    "        unif_samples = np.random.uniform(size = [m])\n",
    "        idx = 0\n",
    "        for g in range(ngroup):\n",
    "            for j in range(group_size[g]):\n",
    "                M = (YtXi[idx] * b[idx] - crossprod(np.delete(beta, idx, axis=0), XmitXi[idx,:]) * b[idx])/sigma2\n",
    "                N = 1/s2 + XitXi[idx] * b[idx]**2 / sigma2\n",
    "                tau_prob[idx] = pi1/( pi1 + 2 * (1-pi1) * (s2*N)**(-0.5)* np.exp(scipy.stats.norm.logcdf(M/np.sqrt(N)) + M**2/(2*N)) )\n",
    "                if (unif_samples[idx] < tau_prob[idx]):\n",
    "                    tau[idx] = 0\n",
    "                else:\n",
    "                    tau[idx] = scipy.stats.truncnorm.rvs(a=-M/N/np.sqrt(1/N), b=np.inf, loc=M/N, scale=np.sqrt(1/N))\n",
    "                    n_nzt = n_nzt + 1\n",
    "                idx = idx + 1\n",
    "\n",
    "        # generate b and compute beta\n",
    "        begin_idx = 0\n",
    "        unif_samples_group = np.random.uniform(size=ngroup)\n",
    "\n",
    "        for g in range(ngroup):\n",
    "            end_idx = begin_idx + group_size[g]\n",
    "            # Covariance Matrix for Group g\n",
    "            Vg = np.diag(tau[begin_idx:end_idx])\n",
    "            Sig = np.linalg.inv(crossprod(Vg, XktXk[g]) @ Vg / sigma2 + np.eye(group_size[g]))\n",
    "            dev = (Vg @ (XktY[g] - XktXmk[g] @ np.delete(beta,np.arange(begin_idx,end_idx), axis=0)))/sigma2\n",
    "            # probability for bg to be 0 vector\n",
    "            b_prob[g] = pi0 / ( pi0 + (1-pi0) * np.linalg.det(Sig)**(0.5) * np.exp(crossprod(dev,Sig)@dev/2) )\n",
    "            if (unif_samples_group[g] < b_prob[g]):\n",
    "                b[begin_idx:end_idx] = 0\n",
    "            else:\n",
    "                b[begin_idx:end_idx] = np.random.multivariate_normal(mean=Sig@dev, cov=Sig)\n",
    "                n_nzg = n_nzg + 1\n",
    "            # Compute beta\n",
    "            beta[begin_idx:end_idx] = Vg @ b[begin_idx:end_idx]\n",
    "            # Update index\n",
    "            begin_idx = end_idx\n",
    "\n",
    "        # store coefficients vector beta\n",
    "        if(iteration - burnin > 0):\n",
    "            coef[:,iteration-burnin] = beta\n",
    "            bprob[:,iteration-burnin] = b_prob\n",
    "            tauprob[:,iteration-burnin] = tau_prob\n",
    "\n",
    "        # Generate sigma2\n",
    "        shape = n/2 + alpha\n",
    "        scale = (YtY + crossprod(beta,XtX)@beta - 2*crossprod(beta, XtY))/2 + gamma\n",
    "        sigma2 = scipy.stats.invgamma.rvs(shape, scale=scale)\n",
    "\n",
    "        if pi_prior:\n",
    "            # Generate pi0\n",
    "            pi0 = scipy.stats.beta.rvs(ngroup - n_nzg + a1, n_nzg + a2)\n",
    "            # Generate pi1\n",
    "            pi1 = scipy.stats.beta.rvs(m - n_nzt + c1, n_nzt + c2)\n",
    "\n",
    "        # Update s2\n",
    "        s2 = scipy.stats.invgamma.rvs(1 + n_nzt/2, scale = t + sum(tau)/2)\n",
    "    \n",
    "    return(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BSGS_SS_simulation(exple, sim_number, train_threshold):\n",
    "    X = exple[\"sim\"][sim_number]['X']\n",
    "    X = X[:int(train_threshold*len(X))]\n",
    "    Y = exple[\"sim\"][sim_number]['Y']\n",
    "    Y = Y[:int(train_threshold*len(Y))]\n",
    "    coef = BSGS_SS(X, Y, exple[\"beg\"])\n",
    "    return([sim_number, coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n"
     ]
    }
   ],
   "source": [
    "nb_runs = 50\n",
    "train_threshold=0.6\n",
    "l_coefs = []\n",
    "\n",
    "results_tpr_fpr_mse = []\n",
    "for number in example:\n",
    "    print(f\"Example {number}\")\n",
    "    exple=example[number]\n",
    "    l_coefs.append( Parallel(n_jobs=10)(delayed(run_BSGS_SS_simulation)\n",
    "        (exple, sim_number, train_threshold)\n",
    "        for sim_number in range(nb_runs))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the mean, median and credible intervals for variable selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "\n",
      "    Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "    Mean FPR=1.0, STD FPR=0.0\n",
      "\n",
      "    Mean MSE=6.834155559935802, STD MSE=2.571479272254547\n",
      "\n",
      "Median:\n",
      "\n",
      "    Mean TPR=0.8098888888888888, STD TPR=0.2468762623033829\n",
      "\n",
      "    Mean FPR=0.01893722943722943, STD FPR=0.05273663714389712\n",
      "\n",
      "    Mean MSE=7.446419333303067, STD MSE=3.379963098913862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_med = []\n",
    "results_mean = []\n",
    "for number in example:\n",
    "    exple = example[number]\n",
    "    beta_star = exple['beta']\n",
    "    for run in range(nb_runs):\n",
    "        sim_number = l_coefs[number-1][run][0]\n",
    "        beta_median = np.median(l_coefs[number-1][run][1], axis=1)\n",
    "        beta_mean = np.mean(l_coefs[number-1][run][1], axis=1)\n",
    "        results_med.append(calc_metrics(exple, sim_number, beta_median,beta_star))\n",
    "        results_mean.append(calc_metrics(exple, sim_number, beta_mean,beta_star))\n",
    "results_mean_all = np.mean(results_mean, axis=0)\n",
    "results_mean_all_std = np.std(results_mean, axis=0)\n",
    "results_med_all = np.mean(results_med, axis=0)\n",
    "results_med_all_std = np.std(results_med, axis=0)\n",
    "\n",
    "print(f\"\"\"Mean:\\n\n",
    "    Mean TPR={results_mean_all[0]}, STD TPR={results_mean_all_std[0]}\\n\n",
    "    Mean FPR={results_mean_all[1]}, STD FPR={results_mean_all_std[1]}\\n\n",
    "    Mean MSE={results_mean_all[2]}, STD MSE={results_mean_all_std[2]}\\n\n",
    "Median:\\n\n",
    "    Mean TPR={results_med_all[0]}, STD TPR={results_med_all_std[0]}\\n\n",
    "    Mean FPR={results_med_all[1]}, STD FPR={results_med_all_std[1]}\\n\n",
    "    Mean MSE={results_med_all[2]}, STD MSE={results_med_all_std[2]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like BGL-SS, the median outperforms the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credibility interval 0.1:\n",
      "\n",
      "        Mean TPR=0.7801666666666667, STD TPR=0.27757076174874373\n",
      "\n",
      "        Mean FPR=0.013268398268398263, STD FPR=0.04421129592571206\n",
      "\n",
      "Credibility interval 0.2:\n",
      "\n",
      "        Mean TPR=0.7566666666666666, STD TPR=0.2981858729296457\n",
      "\n",
      "        Mean FPR=0.010590909090909088, STD FPR=0.037059459888293074\n",
      "\n",
      "Credibility interval 0.3:\n",
      "\n",
      "        Mean TPR=0.7366111111111111, STD TPR=0.31897172269112434\n",
      "\n",
      "        Mean FPR=0.008344155844155841, STD FPR=0.031439420017722396\n",
      "\n",
      "Credibility interval 0.4:\n",
      "\n",
      "        Mean TPR=0.7175, STD TPR=0.33465743955850175\n",
      "\n",
      "        Mean FPR=0.006383116883116883, STD FPR=0.024495165104768367\n",
      "\n",
      "Credibility interval 0.5:\n",
      "\n",
      "        Mean TPR=0.6970555555555557, STD TPR=0.35356469975604315\n",
      "\n",
      "        Mean FPR=0.005093073593073594, STD FPR=0.020361925373352033\n",
      "\n",
      "Credibility interval 0.6:\n",
      "\n",
      "        Mean TPR=0.6786111111111112, STD TPR=0.36848360852701045\n",
      "\n",
      "        Mean FPR=0.003203463203463203, STD FPR=0.016184398238746123\n",
      "\n",
      "Credibility interval 0.7:\n",
      "\n",
      "        Mean TPR=0.6580000000000001, STD TPR=0.38534762244427656\n",
      "\n",
      "        Mean FPR=0.0018896103896103896, STD FPR=0.012756921138624665\n",
      "\n",
      "Credibility interval 0.8:\n",
      "\n",
      "        Mean TPR=0.6413888888888889, STD TPR=0.397056978061285\n",
      "\n",
      "        Mean FPR=0.0009090909090909091, STD FPR=0.009045340337332843\n",
      "\n",
      "Credibility interval 0.9:\n",
      "\n",
      "        Mean TPR=0.6277222222222222, STD TPR=0.40827399039685014\n",
      "\n",
      "        Mean FPR=0.0, STD FPR=0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cred_level in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    eps = 1e-6\n",
    "    results_cred = []\n",
    "    for number in example:\n",
    "        beta_star = example[number]['beta']\n",
    "        for run in range(nb_runs):\n",
    "            cred_lower = np.quantile(l_coefs[number-1][run][1], (1-cred_level)/2, axis=1)\n",
    "            cred_upper = np.quantile(l_coefs[number-1][run][1], (1+cred_level)/2, axis=1)\n",
    "            zero_in_cred = ((cred_lower<=0) & (cred_upper>=0))\n",
    "            \n",
    "            # True Positive Rate = proportion of non-zero coefficients correctly detected \n",
    "            TP=np.where((abs(beta_star)>eps) & (np.logical_not(zero_in_cred)))[0]\n",
    "            # False positives: null coefficients that were estimated as non-zero\n",
    "            FP=np.where((abs(beta_star)<eps) & (np.logical_not(zero_in_cred)))[0]\n",
    "            # true zeros are the coordinates of zeros in the true solution\n",
    "            true_zeros=np.where(beta_star==0)[0]\n",
    "            # true non zeros are the non zeros coordinates of the true solution\n",
    "            true_non_zeros=np.where(beta_star!=0)[0] \n",
    "            TPR=len(TP)/len(true_non_zeros)\n",
    "            # FPR=FP/Negatives\n",
    "            FPR=len(FP)/len(true_zeros)\n",
    "\n",
    "            results_cred.append((TPR,FPR))\n",
    "\n",
    "    results_cred_all = np.mean(results_cred, axis=0)\n",
    "    results_cred_all_std = np.std(results_cred, axis=0)\n",
    "    print(f\"\"\"Credibility interval {cred_level}:\\n\n",
    "        Mean TPR={results_cred_all[0]}, STD TPR={results_cred_all_std[0]}\\n\n",
    "        Mean FPR={results_cred_all[1]}, STD FPR={results_cred_all_std[1]}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using credible intervals we can decrease the FPR to 0 while maintaining decent TPR.\n",
    "\n",
    "Now we use the median beta samples, and calculate error metrics for each example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSGS-SS (Median):\n",
      "Example 1:\n",
      "\n",
      "        Mean TPR=0.47555555555555545, STD TPR=0.1792164013658307\n",
      "\n",
      "        Mean FPR=0.06727272727272725, STD FPR=0.0868360286273422\n",
      "\n",
      "        Median MSE=12.097398909260804, STD MSE=2.5608647790888126\n",
      "\n",
      "Example 2:\n",
      "\n",
      "        Mean TPR=0.7640000000000002, STD TPR=0.15969971822141701\n",
      "\n",
      "        Mean FPR=0.0051428571428571435, STD FPR=0.013327890045360686\n",
      "\n",
      "        Median MSE=6.142610966927522, STD MSE=2.4358864168910754\n",
      "\n",
      "Example 3:\n",
      "\n",
      "        Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "        Mean FPR=0.0, STD FPR=0.0\n",
      "\n",
      "        Median MSE=6.136767876797252, STD MSE=1.6098965754090808\n",
      "\n",
      "Example 4:\n",
      "\n",
      "        Mean TPR=1.0, STD TPR=0.0\n",
      "\n",
      "        Mean FPR=0.0033333333333333335, STD FPR=0.01666666666666667\n",
      "\n",
      "        Median MSE=4.946568043538402, STD MSE=0.9517991104668521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_med_example = [[],[],[],[]]\n",
    "for number in example:\n",
    "    exple = example[number]\n",
    "    beta_star = exple['beta']\n",
    "    for run in range(nb_runs):\n",
    "        sim_number = l_coefs[number-1][run][0]\n",
    "        beta_median = np.median(l_coefs[number-1][run][1], axis=1)\n",
    "        results_med_example[number-1].append(calc_metrics(exple, sim_number, beta_median,beta_star))\n",
    "print(f\"\"\"BSGS-SS (Median):\"\"\")\n",
    "for number in range(4):\n",
    "    results_med_all = np.mean(results_med_example[number], axis=0)\n",
    "    results_med_all_mse = np.median(results_med_example[number], axis=0)\n",
    "    results_med_all_std = np.std(results_med_example[number], axis=0)\n",
    "\n",
    "    print(f\"\"\"Example {number+1}:\\n\n",
    "        Mean TPR={results_med_all[0]}, STD TPR={results_med_all_std[0]}\\n\n",
    "        Mean FPR={results_med_all[1]}, STD FPR={results_med_all_std[1]}\\n\n",
    "        Median MSE={results_med_all_mse[2]}, STD MSE={results_med_all_std[2]}\\n\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de bayesian_stats",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
